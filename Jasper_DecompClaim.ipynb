{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24584,"status":"ok","timestamp":1717077183493,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"},"user_tz":-120},"id":"dJY5W9LiBJdC","outputId":"6b6beea5-c22c-4377-94b1-19762a8b2bed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","import torch\n","import logging\n","logging.basicConfig(level=logging.ERROR)\n","# If there's a GPU available...\n","if torch.cuda.is_available():\n","\n","    # Tell PyTorch to use the GPU.\n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")\n","\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":296274,"status":"ok","timestamp":1717077479753,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"},"user_tz":-120},"id":"rCFaa5PHNgVf","outputId":"72a6c7b0-dbc4-4e8c-c97e-9a7e9b8b3243"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n","Collecting pip\n","  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pip\n","  Attempting uninstall: pip\n","    Found existing installation: pip 23.1.2\n","    Uninstalling pip-23.1.2:\n","      Successfully uninstalled pip-23.1.2\n","Successfully installed pip-24.0\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n","  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mCollecting allennlp\n","  Downloading allennlp-2.10.1-py3-none-any.whl.metadata (21 kB)\n","Collecting torch<1.13.0,>=1.10.0 (from allennlp)\n","  Downloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl.metadata (22 kB)\n","Collecting torchvision<0.14.0,>=0.8.1 (from allennlp)\n","  Downloading torchvision-0.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (10 kB)\n","Collecting cached-path<1.2.0,>=1.1.3 (from allennlp)\n","  Downloading cached_path-1.1.6-py3-none-any.whl.metadata (6.0 kB)\n","Collecting fairscale==0.4.6 (from allennlp)\n","  Downloading fairscale-0.4.6.tar.gz (248 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.2/248.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from allennlp) (3.8.1)\n","Collecting spacy<3.4,>=2.1.0 (from allennlp)\n","  Downloading spacy-3.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\n","Requirement already satisfied: numpy>=1.21.4 in /usr/local/lib/python3.10/dist-packages (from allennlp) (1.25.2)\n","Collecting tensorboardX>=1.2 (from allennlp)\n","  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from allennlp) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62 in /usr/local/lib/python3.10/dist-packages (from allennlp) (4.66.4)\n","Requirement already satisfied: h5py>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from allennlp) (3.9.0)\n","Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from allennlp) (1.2.2)\n","Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from allennlp) (1.11.4)\n","Requirement already satisfied: pytest>=6.2.5 in /usr/local/lib/python3.10/dist-packages (from allennlp) (7.4.4)\n","Collecting transformers<4.21,>=4.1 (from allennlp)\n","  Downloading transformers-4.20.1-py3-none-any.whl.metadata (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.3/77.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sentencepiece>=0.1.96 in /usr/local/lib/python3.10/dist-packages (from allennlp) (0.1.99)\n","Collecting filelock<3.8,>=3.3 (from allennlp)\n","  Downloading filelock-3.7.1-py3-none-any.whl.metadata (2.5 kB)\n","Collecting lmdb>=1.2.1 (from allennlp)\n","  Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n","Requirement already satisfied: more-itertools>=8.12.0 in /usr/local/lib/python3.10/dist-packages (from allennlp) (10.1.0)\n","Collecting termcolor==1.1.0 (from allennlp)\n","  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting wandb<0.13.0,>=0.10.0 (from allennlp)\n","  Downloading wandb-0.12.21-py2.py3-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: huggingface-hub>=0.0.16 in /usr/local/lib/python3.10/dist-packages (from allennlp) (0.23.1)\n","Collecting dill>=0.3.4 (from allennlp)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Collecting base58>=2.1.1 (from allennlp)\n","  Downloading base58-2.1.1-py3-none-any.whl.metadata (3.1 kB)\n","Collecting sacremoses (from allennlp)\n","  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n","Requirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from allennlp) (0.9.4)\n","Requirement already satisfied: protobuf<4.0.0,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from allennlp) (3.20.3)\n","Requirement already satisfied: traitlets>5.1.1 in /usr/local/lib/python3.10/dist-packages (from allennlp) (5.7.1)\n","Collecting jsonnet>=0.10.0 (from allennlp)\n","  Downloading jsonnet-0.20.0.tar.gz (594 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.2/594.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting rich<13.0,>=12.1 (from cached-path<1.2.0,>=1.1.3->allennlp)\n","  Downloading rich-12.6.0-py3-none-any.whl.metadata (18 kB)\n","Collecting boto3<2.0,>=1.0 (from cached-path<1.2.0,>=1.1.3->allennlp)\n","  Downloading boto3-1.34.115-py3-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (2.8.0)\n","Collecting huggingface-hub>=0.0.16 (from allennlp)\n","  Downloading huggingface_hub-0.10.1-py3-none-any.whl.metadata (6.1 kB)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.16->allennlp) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.16->allennlp) (4.11.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.16->allennlp) (24.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->allennlp) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->allennlp) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->allennlp) (2024.5.15)\n","Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp) (2.0.0)\n","Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp) (1.5.0)\n","Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp) (1.2.1)\n","Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp) (2.0.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp) (2024.2.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.1->allennlp) (3.5.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.9)\n","Collecting thinc<8.1.0,>=8.0.14 (from spacy<3.4,>=2.1.0->allennlp)\n","  Downloading thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.7.11)\n","Collecting wasabi<1.1.0,>=0.9.1 (from spacy<3.4,>=2.1.0->allennlp)\n","  Downloading wasabi-0.10.1-py3-none-any.whl.metadata (28 kB)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.10)\n","Collecting typer>=0.4.1 (from allennlp)\n","  Downloading typer-0.4.2-py3-none-any.whl.metadata (12 kB)\n","Collecting pathy>=0.3.5 (from spacy<3.4,>=2.1.0->allennlp)\n","  Downloading pathy-0.11.0-py3-none-any.whl.metadata (16 kB)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (6.4.0)\n","Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 (from spacy<3.4,>=2.1.0->allennlp)\n","  Downloading pydantic-1.8.2-py3-none-any.whl.metadata (103 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (67.7.2)\n","Collecting typing-extensions>=3.7.4.3 (from huggingface-hub>=0.0.16->allennlp)\n","  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.4.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision<0.14.0,>=0.8.1->allennlp) (9.4.0)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers<4.21,>=4.1->allennlp)\n","  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.5 kB)\n","Collecting GitPython>=1.0.0 (from wandb<0.13.0,>=0.10.0->allennlp)\n","  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.3)\n","Collecting shortuuid>=0.5.0 (from wandb<0.13.0,>=0.10.0->allennlp)\n","  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.9.5)\n","Collecting sentry-sdk>=1.0.0 (from wandb<0.13.0,>=0.10.0->allennlp)\n","  Downloading sentry_sdk-2.3.1-py2.py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.16.0)\n","Collecting docker-pycreds>=0.4.0 (from wandb<0.13.0,>=0.10.0->allennlp)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n","Collecting pathtools (from wandb<0.13.0,>=0.10.0->allennlp)\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting setproctitle (from wandb<0.13.0,>=0.10.0->allennlp)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n","Collecting botocore<1.35.0,>=1.34.115 (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp)\n","  Downloading botocore-1.34.115-py3-none-any.whl.metadata (5.7 kB)\n","Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp)\n","  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n","Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp)\n","  Downloading s3transfer-0.10.1-py3-none-any.whl.metadata (1.7 kB)\n","Collecting gitdb<5,>=4.0.1 (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp)\n","  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n","Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.27.0)\n","Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.11.1)\n","Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.3.3)\n","Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.7.0)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.4,>=2.1.0->allennlp) (1.2.0)\n","Collecting pathlib-abc==0.1.1 (from pathy>=0.3.5->spacy<3.4,>=2.1.0->allennlp)\n","  Downloading pathlib_abc-0.1.1-py3-none-any.whl.metadata (18 kB)\n","Collecting commonmark<0.10.0,>=0.9.0 (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp)\n","  Downloading commonmark-0.9.1-py2.py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp) (2.16.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp) (2.1.5)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.115->boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.8.2)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp)\n","  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n","Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.63.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (4.9)\n","Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.5.0)\n","Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.4,>=2.1.0->allennlp) (1.1.1)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.6.0)\n","Downloading allennlp-2.10.1-py3-none-any.whl (730 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m730.2/730.2 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n","Downloading cached_path-1.1.6-py3-none-any.whl (26 kB)\n","Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading filelock-3.7.1-py3-none-any.whl (10 kB)\n","Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading spacy-3.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchvision-0.13.1-cp310-cp310-manylinux1_x86_64.whl (19.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typer-0.4.2-py3-none-any.whl (27 kB)\n","Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading boto3-1.34.115-py3-none-any.whl (139 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pathy-0.11.0-py3-none-any.whl (47 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pathlib_abc-0.1.1-py3-none-any.whl (23 kB)\n","Downloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rich-12.6.0-py3-none-any.whl (237 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.5/237.5 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sentry_sdk-2.3.1-py2.py3-none-any.whl (289 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.0/289.0 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n","Downloading thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (659 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m659.5/659.5 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n","Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n","Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Downloading botocore-1.34.115-py3-none-any.whl (12.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Building wheels for collected packages: fairscale, termcolor, jsonnet, pathtools\n","  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307222 sha256=7de0c5959cc17ced7058005b45867d4ff0d3bd958ced0077ae32b273deeaefda\n","  Stored in directory: /root/.cache/pip/wheels/a1/58/3d/e114952ab4a8f31eb9dae230658450afff986b211a5b1f2256\n","  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=362a7b36222b8ab19e8cdd98f0175cf17181a9103bef1546a45f8e16ce7cb418\n","  Stored in directory: /root/.cache/pip/wheels/a1/49/46/1b13a65d8da11238af9616b00fdde6d45b0f95d9291bac8452\n","  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jsonnet: filename=jsonnet-0.20.0-cp310-cp310-linux_x86_64.whl size=6406866 sha256=f7427fa2385ae02e2cce9c9eb03a1c203f861e2267da2297233d38d49d2c25be\n","  Stored in directory: /root/.cache/pip/wheels/63/0d/6b/5467dd1db9332ba4bd5cf4153e2870c5f89bb4db473d989cc2\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=8c4cadfd66ba0f5a006ac79b924fa62eff671cda71c1de7ca0cc2b905374f842\n","  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n","Successfully built fairscale termcolor jsonnet pathtools\n","Installing collected packages: wasabi, tokenizers, termcolor, pathtools, lmdb, jsonnet, commonmark, typing-extensions, typer, tensorboardX, smmap, shortuuid, setproctitle, sentry-sdk, sacremoses, rich, pathlib-abc, jmespath, filelock, docker-pycreds, dill, base58, torch, pydantic, pathy, huggingface-hub, gitdb, botocore, transformers, torchvision, thinc, s3transfer, GitPython, fairscale, wandb, spacy, boto3, cached-path, allennlp\n","  Attempting uninstall: wasabi\n","    Found existing installation: wasabi 1.1.2\n","    Uninstalling wasabi-1.1.2:\n","      Successfully uninstalled wasabi-1.1.2\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.19.1\n","    Uninstalling tokenizers-0.19.1:\n","      Successfully uninstalled tokenizers-0.19.1\n","  Attempting uninstall: termcolor\n","    Found existing installation: termcolor 2.4.0\n","    Uninstalling termcolor-2.4.0:\n","      Successfully uninstalled termcolor-2.4.0\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.11.0\n","    Uninstalling typing_extensions-4.11.0:\n","      Successfully uninstalled typing_extensions-4.11.0\n","  Attempting uninstall: typer\n","    Found existing installation: typer 0.9.4\n","    Uninstalling typer-0.9.4:\n","      Successfully uninstalled typer-0.9.4\n","  Attempting uninstall: rich\n","    Found existing installation: rich 13.7.1\n","    Uninstalling rich-13.7.1:\n","      Successfully uninstalled rich-13.7.1\n","  Attempting uninstall: filelock\n","    Found existing installation: filelock 3.14.0\n","    Uninstalling filelock-3.14.0:\n","      Successfully uninstalled filelock-3.14.0\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.3.0+cu121\n","    Uninstalling torch-2.3.0+cu121:\n","      Successfully uninstalled torch-2.3.0+cu121\n","  Attempting uninstall: pydantic\n","    Found existing installation: pydantic 2.7.1\n","    Uninstalling pydantic-2.7.1:\n","      Successfully uninstalled pydantic-2.7.1\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.23.1\n","    Uninstalling huggingface-hub-0.23.1:\n","      Successfully uninstalled huggingface-hub-0.23.1\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.41.1\n","    Uninstalling transformers-4.41.1:\n","      Successfully uninstalled transformers-4.41.1\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.18.0+cu121\n","    Uninstalling torchvision-0.18.0+cu121:\n","      Successfully uninstalled torchvision-0.18.0+cu121\n","  Attempting uninstall: thinc\n","    Found existing installation: thinc 8.2.3\n","    Uninstalling thinc-8.2.3:\n","      Successfully uninstalled thinc-8.2.3\n","  Attempting uninstall: spacy\n","    Found existing installation: spacy 3.7.4\n","    Uninstalling spacy-3.7.4:\n","      Successfully uninstalled spacy-3.7.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","sqlalchemy 2.0.30 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n","en-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.3.3 which is incompatible.\n","inflect 7.0.0 requires pydantic>=1.9.1, but you have pydantic 1.8.2 which is incompatible.\n","pydantic-core 2.18.2 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n","torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 1.12.1 which is incompatible.\n","torchtext 0.18.0 requires torch>=2.3.0, but you have torch 1.12.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed GitPython-3.1.43 allennlp-2.10.1 base58-2.1.1 boto3-1.34.115 botocore-1.34.115 cached-path-1.1.6 commonmark-0.9.1 dill-0.3.8 docker-pycreds-0.4.0 fairscale-0.4.6 filelock-3.7.1 gitdb-4.0.11 huggingface-hub-0.10.1 jmespath-1.0.1 jsonnet-0.20.0 lmdb-1.4.1 pathlib-abc-0.1.1 pathtools-0.1.2 pathy-0.11.0 pydantic-1.8.2 rich-12.6.0 s3transfer-0.10.1 sacremoses-0.1.1 sentry-sdk-2.3.1 setproctitle-1.3.3 shortuuid-1.0.13 smmap-5.0.1 spacy-3.3.3 tensorboardX-2.6.2.2 termcolor-1.1.0 thinc-8.0.17 tokenizers-0.12.1 torch-1.12.1 torchvision-0.13.1 transformers-4.20.1 typer-0.4.2 typing-extensions-4.5.0 wandb-0.12.21 wasabi-0.10.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["torch","torchgen"]},"id":"a1f5e03a14ad431fae3cd61ae766cd6f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m^C\n"]}],"source":["!pip install --upgrade pip\n","\n","!pip install torch\n","!pip install allennlp\n","!pip install tqdm pandas beautifulsoup4 requests\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1717077479754,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"},"user_tz":-120},"id":"dcFlod2NqaLQ","outputId":"26d89413-cfa0-4217-d2f8-9dd4095798f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1ddSDwiHDKf6ffm9NsuyXSK6uep9cGT6k/NLP Group Project/claimDecomp\n"]}],"source":["# Change directory to the target folder\n","%cd /content/drive/MyDrive/TUDelft/NLP Group Project/claimDecomp\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"7rWspLUk6o5g","executionInfo":{"status":"ok","timestamp":1717077479754,"user_tz":-120,"elapsed":3,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}}},"outputs":[],"source":["# # Execute the bash script\n","# !bash scripts/download_data.sh"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"wV8mjjceqcmA","executionInfo":{"status":"ok","timestamp":1717077479754,"user_tz":-120,"elapsed":3,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}}},"outputs":[],"source":["# %cd /content/drive/MyDrive/TUDelft/NLP Group Project/claimDecomp/scripts\n","# !ls\n","# # !bash run_nli_models.sh"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xy53ORzg235B"},"outputs":[],"source":["from google.colab import drive\n","\n","# Step 1: Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Step 2: Change directory to the target folder\n","%cd /content/drive/MyDrive/TUDelft/NLP Group Project/claimDecomp/scripts\n","\n","# Step 3: Install the compatible versions of torch and allennlp\n","\n","\n","# Step 4: List the contents to verify the script is present\n","!ls\n","\n","# # Step 5: Run the Python script directly\n","# !python3 run_nli.py\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s0V2SETTy1Uh"},"outputs":[],"source":["import numpy\n","from overrides import overrides\n","from typing import List, Dict\n","from allennlp.common.util import JsonDict\n","from allennlp.data import Instance\n","from allennlp.predictors.predictor import Predictor\n","from allennlp.data.fields import LabelField\n","\n","\n","@Predictor.register(\"doc_nli\")\n","class DocNliPredictor(Predictor):\n","\n","    def predict(self, premise: str, hypothesis: str,\n","                answer_score: float = None) -> JsonDict:\n","        instance = self._dataset_reader.text_to_instance(\n","            premise,\n","            hypothesis,\n","            answer_score\n","        )\n","        if instance:\n","            return self.predict_instance(instance)\n","        else:\n","            return None\n","\n","    def predict_batch(self, premises: List[str], hypothesises: List[str],\n","                      answer_scores: List[float] = None):\n","        instances = []\n","        if answer_scores is None:\n","            for premise, hypothesis in zip(premises, hypothesises):\n","                instance = self._dataset_reader.text_to_instance(\n","                    premise,\n","                    hypothesis\n","                )\n","                if instance:\n","                    instances.append(instance)\n","        else:\n","            for premise, hypothesis, answer_score in zip(premises,\n","                                                         hypothesises,\n","                                                         answer_scores\n","                                                         ):\n","                instance = self._dataset_reader.text_to_instance(\n","                    premise,\n","                    hypothesis,\n","                    answer_score=answer_score\n","                )\n","                if instance:\n","                    instances.append(instance)\n","        outputs = self.predict_batch_instance(instances)\n","        return outputs\n","\n","    @overrides\n","    def predictions_to_labeled_instances(\n","            self, instance: Instance, outputs: Dict[str, numpy.ndarray]\n","    ) -> List[Instance]:\n","        # This function is used to to compute gradients of what the model predicted.\n","        new_instance = instance.duplicate()\n","        label = numpy.argmax(outputs[\"logits\"])\n","        # Skip indexing, we have integer representations of the strings \"entailment\", etc.\n","        new_instance.add_field(\"label\",\n","                               LabelField(int(label), skip_indexing=True))\n","        return [new_instance]\n","\n","    @overrides\n","    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n","        premise = json_dict[\"premise\"]\n","        hypothesis = json_dict[\"hypothesis\"]\n","        instance = self._dataset_reader.text_to_instance(\n","            premise,\n","            hypothesis\n","        )\n","        return instance\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t4vMn14gM4BG"},"outputs":[],"source":["from typing import Dict, Optional\n","import json\n","import logging\n","from overrides import overrides\n","\n","from allennlp.common.file_utils import cached_path\n","from allennlp.common.util import pad_sequence_to_length\n","from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n","from allennlp.data.fields import Field, TextField, LabelField, MetadataField, ArrayField\n","from allennlp.data.instance import Instance\n","from allennlp.data.token_indexers import SingleIdTokenIndexer, TokenIndexer\n","from allennlp.data.tokenizers import Tokenizer, SpacyTokenizer, PretrainedTransformerTokenizer\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","@DatasetReader.register(\"doc_nli\")\n","class DocNliReader(DatasetReader):\n","\n","    def __init__(\n","        self,\n","        tokenizer: Optional[Tokenizer] = None,\n","        token_indexers: Dict[str, TokenIndexer] = None,\n","        max_source_length: Optional[int] = 512,\n","        **kwargs,\n","    ) -> None:\n","        super().__init__(manual_distributed_sharding=True, **kwargs)\n","        self._tokenizer = tokenizer or SpacyTokenizer()\n","        if isinstance(self._tokenizer, PretrainedTransformerTokenizer):\n","            assert not self._tokenizer._add_special_tokens\n","        self._token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n","        self.max_source_length = max_source_length\n","\n","    @overrides\n","    def _read(self, file_path: str):\n","        file_path = cached_path(file_path)\n","\n","        with open(file_path, \"r\") as doc_nli_file:\n","            doc_nli_examples = json.load(doc_nli_file)\n","            count = 0\n","            for example in doc_nli_examples:\n","                label = \"entail\" if example[\"label\"] == 'entailment' else \"not_entail\"\n","                # using the whole paragraph as premise or just the answering sent\n","                premise = example['premise']\n","                count += 1\n","                # if self.joint_training and count == 1000:\n","                #     break\n","                hypothesis = example[\"hypothesis\"]\n","                instance = self.text_to_instance(premise,\n","                                                 hypothesis,\n","                                                 label,\n","                                                 )\n","                if instance:\n","                    yield instance\n","\n","    @overrides\n","    def text_to_instance(\n","        self,  # type: ignore\n","        premise: str,\n","        hypothesis: str,\n","        label: str = None,\n","        answer_score: float = None\n","    ) -> Instance:\n","        fields: Dict[str, Field] = {}\n","        premise = self._tokenizer.tokenize(premise)\n","        hypothesis = self._tokenizer.tokenize(hypothesis)\n","        tokens = self._tokenizer.add_special_tokens(premise, hypothesis)\n","\n","        if len(tokens) > self.max_source_length:\n","            tokens = tokens[:self.max_source_length]\n","        fields[\"tokens\"] = TextField(tokens, self._token_indexers)\n","\n","        metadata = {\n","            \"premise_tokens\": [x.text for x in premise],\n","            \"hypothesis_tokens\": [x.text for x in hypothesis],\n","        }\n","        fields[\"metadata\"] = MetadataField(metadata)\n","\n","        if label:\n","            fields[\"label\"] = LabelField(label)\n","\n","        return Instance(fields)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aiSL9UWGM7E4"},"outputs":[],"source":["from typing import Dict, Optional\n","\n","from overrides import overrides\n","import torch\n","\n","from allennlp.data import TextFieldTensors, Vocabulary\n","from allennlp.models.model import Model\n","from allennlp.modules import FeedForward, Seq2SeqEncoder, Seq2VecEncoder, TextFieldEmbedder\n","from allennlp.nn import InitializerApplicator, util, Activation\n","from allennlp.nn.util import get_text_field_mask\n","from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n","from allennlp.common import Params\n","\n","\n","@Model.register(\"qa_nli_classifier\")\n","class QaNliClassifier(Model):\n","    \"\"\"\n","    This `Model` implements a basic text classifier. After embedding the text into\n","    a text field, we will optionally encode the embeddings with a `Seq2SeqEncoder`. The\n","    resulting sequence is pooled using a `Seq2VecEncoder` and then passed to\n","    a linear classification layer, which projects into the label space. If a\n","    `Seq2SeqEncoder` is not provided, we will pass the embedded text directly to the\n","    `Seq2VecEncoder`.\n","    Registered as a `Model` with name \"basic_classifier\".\n","    # Parameters\n","    vocab : `Vocabulary`\n","    text_field_embedder : `TextFieldEmbedder`\n","        Used to embed the input text into a `TextField`\n","    seq2seq_encoder : `Seq2SeqEncoder`, optional (default=`None`)\n","        Optional Seq2Seq encoder layer for the input text.\n","    seq2vec_encoder : `Seq2VecEncoder`\n","        Required Seq2Vec encoder layer. If `seq2seq_encoder` is provided, this encoder\n","        will pool its output. Otherwise, this encoder will operate directly on the output\n","        of the `text_field_embedder`.\n","    feedforward : `FeedForward`, optional, (default = `None`)\n","        An optional feedforward layer to apply after the seq2vec_encoder.\n","    dropout : `float`, optional (default = `None`)\n","        Dropout percentage to use.\n","    num_labels : `int`, optional (default = `None`)\n","        Number of labels to project to in classification layer. By default, the classification layer will\n","        project to the size of the vocabulary namespace corresponding to labels.\n","    label_namespace : `str`, optional (default = `\"labels\"`)\n","        Vocabulary namespace corresponding to labels. By default, we use the \"labels\" namespace.\n","    initializer : `InitializerApplicator`, optional (default=`InitializerApplicator()`)\n","        If provided, will be used to initialize the model parameters.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        vocab: Vocabulary,\n","        text_field_embedder: TextFieldEmbedder,\n","        seq2vec_encoder: Seq2VecEncoder,\n","        seq2seq_encoder: Seq2SeqEncoder = None,\n","        feedforward: Optional[FeedForward] = None,\n","        dropout: float = None,\n","        num_labels: int = None,\n","        label_namespace: str = \"labels\",\n","        namespace: str = \"tokens\",\n","        initializer: Optional[InitializerApplicator] = None,\n","        use_answer_score: Optional[bool] = None,\n","        **kwargs,\n","    ) -> None:\n","\n","        super().__init__(vocab, **kwargs)\n","        self._text_field_embedder = text_field_embedder\n","        self._seq2seq_encoder = seq2seq_encoder\n","        self._seq2vec_encoder = seq2vec_encoder\n","\n","        self._use_answer_score = use_answer_score\n","\n","        params = Params({\n","            'input_dim': 1025 if use_answer_score else 1024,\n","            'hidden_dims': 1025 if use_answer_score else 1024,\n","            'activations': 'tanh',\n","            'num_layers': 1\n","        })\n","\n","        self._feedforward = FeedForward.from_params(params)\n","\n","        if feedforward is not None:\n","            self._classifier_input_dim = feedforward.get_output_dim()\n","        else:\n","            self._classifier_input_dim = self._seq2vec_encoder.get_output_dim()\n","        if self._use_answer_score:\n","            self._classifier_input_dim += 1\n","\n","\n","        if dropout:\n","            self._dropout = torch.nn.Dropout(dropout)\n","        else:\n","            self._dropout = None\n","        self._label_namespace = label_namespace\n","        self._namespace = namespace\n","\n","        if num_labels:\n","            self._num_labels = num_labels\n","        else:\n","            self._num_labels = vocab.get_vocab_size(namespace=self._label_namespace)\n","\n","        self._classification_layer = torch.nn.Linear(self._classifier_input_dim, self._num_labels)\n","        self._accuracy = CategoricalAccuracy()\n","        self._f1 = F1Measure(positive_label=0)\n","        self._loss = torch.nn.CrossEntropyLoss()\n","        if initializer is not None:\n","            initializer(self)\n","\n","    def forward(  # type: ignore\n","        self,\n","        tokens: TextFieldTensors,\n","        label: torch.IntTensor = None,\n","        answer_scores: torch.FloatTensor = None,\n","        metadata: Dict = None,\n","    ) -> Dict[str, torch.Tensor]:\n","\n","        \"\"\"\n","        # Parameters\n","        tokens : `TextFieldTensors`\n","            From a `TextField`\n","        label : `torch.IntTensor`, optional (default = `None`)\n","            From a `LabelField`\n","        # Returns\n","        An output dictionary consisting of:\n","            - `logits` (`torch.FloatTensor`) :\n","                A tensor of shape `(batch_size, num_labels)` representing\n","                unnormalized log probabilities of the label.\n","            - `probs` (`torch.FloatTensor`) :\n","                A tensor of shape `(batch_size, num_labels)` representing\n","                probabilities of the label.\n","            - `loss` : (`torch.FloatTensor`, optional) :\n","                A scalar loss to be optimised.\n","        \"\"\"\n","\n","        embedded_text = self._text_field_embedder(tokens)\n","        mask = get_text_field_mask(tokens)\n","        if self._seq2seq_encoder:\n","            embedded_text = self._seq2seq_encoder(embedded_text, mask=mask)\n","\n","        embedded_text = self._seq2vec_encoder(embedded_text, mask=mask)\n","\n","        if self._dropout:\n","            embedded_text = self._dropout(embedded_text)\n","\n","        if self._use_answer_score:\n","            answer_scores = torch.unsqueeze(answer_scores, 1)\n","            embedded_text = torch.cat([embedded_text, answer_scores],\n","                                      dim=-1)\n","\n","        if self._feedforward is not None:\n","            embedded_text = self._feedforward(embedded_text)\n","\n","        logits = self._classification_layer(embedded_text)\n","        probs = torch.nn.functional.softmax(logits, dim=-1)\n","\n","        output_dict = {\"logits\": logits, \"probs\": probs,\n","                       \"token_ids\": util.get_token_ids_from_text_field_tensors(\n","                           tokens)}\n","        self.make_output_human_readable(output_dict)\n","        if label is not None:\n","            loss = self._loss(logits, label.long().view(-1))\n","            output_dict[\"loss\"] = loss\n","            self._accuracy(logits, label)\n","            self._f1(logits, label)\n","\n","        return output_dict\n","\n","    @overrides\n","    def make_output_human_readable(\n","        self, output_dict: Dict[str, torch.Tensor]\n","    ) -> Dict[str, torch.Tensor]:\n","        \"\"\"\n","        Does a simple argmax over the probabilities, converts index to string label, and\n","        add `\"label\"` key to the dictionary with the result.\n","        \"\"\"\n","        predictions = output_dict[\"probs\"]\n","        if predictions.dim() == 2:\n","            predictions_list = [predictions[i] for i in range(predictions.shape[0])]\n","        else:\n","            predictions_list = [predictions]\n","        classes = []\n","        for prediction in predictions_list:\n","            label_idx = prediction.argmax(dim=-1).item()\n","            label_str = self.vocab.get_index_to_token_vocabulary(self._label_namespace).get(\n","                label_idx, str(label_idx)\n","            )\n","            classes.append(label_str)\n","        output_dict[\"label\"] = classes\n","        tokens = []\n","        for instance_tokens in output_dict[\"token_ids\"]:\n","            tokens.append(\n","                [\n","                    self.vocab.get_token_from_index(token_id.item(), namespace=self._namespace)\n","                    for token_id in instance_tokens\n","                ]\n","            )\n","        output_dict[\"tokens\"] = tokens\n","        return output_dict\n","\n","    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n","        acc = self._accuracy.get_metric(reset)\n","        f1_metric = self._f1.get_metric(reset)\n","        metrics = {\"accuracy\": acc,\n","                   \"precision\": f1_metric['precision'],\n","                   \"recall\": f1_metric['recall'],\n","                   \"f1\": f1_metric['f1']}\n","        return metrics\n","\n","    default_predictor = \"text_classifier\""]},{"cell_type":"markdown","metadata":{"id":"tFwy38XHy3tV"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SGH7rMtWCUO_"},"outputs":[],"source":["import argparse\n","from allennlp.common.util import import_module_and_submodules\n","from allennlp.predictors.predictor import Predictor\n","\n","\n","def main(args):\n","    if args.model == \"nq-nli\":\n","        model_path = \"./nli_models/nq-nli.tar.gz\"\n","        model_name = \"qa_nli\"\n","    elif args.model == 'mnli':\n","        model_path = \"./nli_models/mnli.tar.gz\"\n","        model_name = \"textual_entailment\"\n","    elif args.model == 'doc-nli':\n","        model_path = \"./nli_models/doc-nli.tar.gz\"\n","        model_name = \"qa_nli\"\n","    else:\n","        raise ValueError('no model named {}'.format(args.model))\n","\n","    predictor = Predictor.from_path(\n","        model_path,\n","        model_name,\n","        cuda_device=0)\n","\n","    premise = \"The jobless rate for Hispanics hit a record low of 3.9% in\" \\\n","              \" September, while African Americans maintained its lowest rate \" \\\n","              \"ever, 5.5%.\"\n","    hypothesis = \"The unemployment rate of African Americans is historically low.\"\n","\n","    results = predictor.predict(\n","        premise=premise,\n","        hypothesis=hypothesis\n","    )\n","    print(results)\n","\n","\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--model', type=str, default='doc_nli')\n","    parsed_args = parser.parse_args()\n","    import_module_and_submodules(\"predictors\")\n","    import_module_and_submodules(\"dataset_reader\")\n","    import_module_and_submodules(\"model\")\n","    main(parsed_args)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-6FJ129FDwAq"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyOchAleBDuGH5MV6s1mUtpB"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}