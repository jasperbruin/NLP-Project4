{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1gZJCakmY28cKGMj8B7wd1GUM3r72pdbi","timestamp":1716239074797},{"file_id":"1LsplV5BkBuP399HR8PhVNET_qLGnHXgA","timestamp":1713519252688},{"file_id":"1ronZyGW5kwwOCNkRqNWYqg6f4neyA7h5","timestamp":1700999246067}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"5fbb2c4b9cb04587a15bcdbf0db0991b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_27c7ce7ab9704485b280ee214c5f410b","IPY_MODEL_962e795bdbba4c0f8b5ec41794df47ef","IPY_MODEL_5c7e7b7b326e4cc0ab30a06e0fe4c381"],"layout":"IPY_MODEL_71c06e7be8d548fa82ebdd9a3d46ecbe"}},"27c7ce7ab9704485b280ee214c5f410b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6c0f4cf13994489b5c5b464bd3b39f7","placeholder":"​","style":"IPY_MODEL_ff4ec3dc486749a2aee99f7053b47425","value":"tokenizer_config.json: 100%"}},"962e795bdbba4c0f8b5ec41794df47ef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_52c9486d28e040de86d3fbae6df4241a","max":26,"min":0,"orientation":"horizontal","style":"IPY_MODEL_04b3821d4ef84daca8231562f80c0a99","value":26}},"5c7e7b7b326e4cc0ab30a06e0fe4c381":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4af3ed29094d415c941e0e194fa3ec73","placeholder":"​","style":"IPY_MODEL_049288d6594b43f5ac6bafddfe4984d1","value":" 26.0/26.0 [00:00&lt;00:00, 1.28kB/s]"}},"71c06e7be8d548fa82ebdd9a3d46ecbe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6c0f4cf13994489b5c5b464bd3b39f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff4ec3dc486749a2aee99f7053b47425":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52c9486d28e040de86d3fbae6df4241a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04b3821d4ef84daca8231562f80c0a99":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4af3ed29094d415c941e0e194fa3ec73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"049288d6594b43f5ac6bafddfe4984d1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"849c112973954f6c9572da74704fa28f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f8c997705d7a476f90e0ac8d17d4a662","IPY_MODEL_d029681583684c509d52b9efb2baa1d8","IPY_MODEL_a8ec6c0335fd40e4a41a5c767b6b8c18"],"layout":"IPY_MODEL_e943f87d6171475f83b97b4f8506e4d0"}},"f8c997705d7a476f90e0ac8d17d4a662":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b60fb49a79645abb10ca44635398c1c","placeholder":"​","style":"IPY_MODEL_d8af21b9261f4998b6cea145f03913f8","value":"config.json: 100%"}},"d029681583684c509d52b9efb2baa1d8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa6939c5c21c4bc5bca32b5e53e0695f","max":1154,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d07a75c096ac4bd2b351ca87d0feb0fb","value":1154}},"a8ec6c0335fd40e4a41a5c767b6b8c18":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1177a94be5dd4ecdb408413502dd6951","placeholder":"​","style":"IPY_MODEL_97838f17e50f4e6cb6106f943bdccace","value":" 1.15k/1.15k [00:00&lt;00:00, 55.5kB/s]"}},"e943f87d6171475f83b97b4f8506e4d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b60fb49a79645abb10ca44635398c1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8af21b9261f4998b6cea145f03913f8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fa6939c5c21c4bc5bca32b5e53e0695f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d07a75c096ac4bd2b351ca87d0feb0fb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1177a94be5dd4ecdb408413502dd6951":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97838f17e50f4e6cb6106f943bdccace":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ef4b7408efa041bba4e1cfe22ecc750a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_85e9eec63bd24333a19d37f0aed44a2e","IPY_MODEL_65ba51616a9d467b84127c438e017f66","IPY_MODEL_91bc7f438d7c42d2a08a01d4b357f22e"],"layout":"IPY_MODEL_846bbcfff48a4bd682c9c805529d8c75"}},"85e9eec63bd24333a19d37f0aed44a2e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8333cc5d74746d5b3ba70cccc660c4e","placeholder":"​","style":"IPY_MODEL_a7df96120aa04add92849c198122b809","value":"vocab.json: 100%"}},"65ba51616a9d467b84127c438e017f66":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a7103c7bdbf64ee0bcfa7af62bffd76f","max":898822,"min":0,"orientation":"horizontal","style":"IPY_MODEL_15f015f07333490f8417e15b5abb17b6","value":898822}},"91bc7f438d7c42d2a08a01d4b357f22e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_66928b4933b741558bf480b192b2427d","placeholder":"​","style":"IPY_MODEL_4986dfd9775547b6b53244378a9d9225","value":" 899k/899k [00:00&lt;00:00, 4.61MB/s]"}},"846bbcfff48a4bd682c9c805529d8c75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8333cc5d74746d5b3ba70cccc660c4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7df96120aa04add92849c198122b809":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a7103c7bdbf64ee0bcfa7af62bffd76f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15f015f07333490f8417e15b5abb17b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"66928b4933b741558bf480b192b2427d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4986dfd9775547b6b53244378a9d9225":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4bd2a111cf804bbb96c16f4b0fb3822e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_edfbf3c80ebf4071af271831993f0cef","IPY_MODEL_41afeed0335341a9b3a8559843712d84","IPY_MODEL_39919a9b549a4e0ba984b81b7e4d805d"],"layout":"IPY_MODEL_b77fd1991e064a68978ea9065f3a4ea6"}},"edfbf3c80ebf4071af271831993f0cef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd753754cfda4064a324910b939b674d","placeholder":"​","style":"IPY_MODEL_bef297dbd4444242908fcb5b076e6d56","value":"merges.txt: 100%"}},"41afeed0335341a9b3a8559843712d84":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a1700d2fec2440092e5077bcf5094a3","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_04db08d11f644985abc5f7a001fa46e3","value":456318}},"39919a9b549a4e0ba984b81b7e4d805d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89cdce60ace64a01b8b39ac29b355066","placeholder":"​","style":"IPY_MODEL_4adc7602628c4020ba1bde5c0c17cc75","value":" 456k/456k [00:00&lt;00:00, 10.9MB/s]"}},"b77fd1991e064a68978ea9065f3a4ea6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd753754cfda4064a324910b939b674d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bef297dbd4444242908fcb5b076e6d56":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a1700d2fec2440092e5077bcf5094a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04db08d11f644985abc5f7a001fa46e3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"89cdce60ace64a01b8b39ac29b355066":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4adc7602628c4020ba1bde5c0c17cc75":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6cb347273df845fbbb0cc35b0cb93491":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_94683ee068234798ba94227ff2c3f50b","IPY_MODEL_e93a5867d7f2431da1eaacc9a9c63af5","IPY_MODEL_fcf8eeb4fdf34a249e3694231f90131e"],"layout":"IPY_MODEL_e5ceb2ac93624f2fb1d1c846daf4429f"}},"94683ee068234798ba94227ff2c3f50b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_93464a7f4d9441b0bb2a43f06b80b2c5","placeholder":"​","style":"IPY_MODEL_b7e7966d70dd4da2b349fe852eac8fad","value":"tokenizer.json: 100%"}},"e93a5867d7f2431da1eaacc9a9c63af5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d097a8d391241adbed14148c2334573","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a63785f55a6f4e7989771f2a7f166c3f","value":1355863}},"fcf8eeb4fdf34a249e3694231f90131e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_47aa34356529466aa342dd3e897b4a5f","placeholder":"​","style":"IPY_MODEL_92df99d330b74983987c51a04309f9a3","value":" 1.36M/1.36M [00:00&lt;00:00, 19.7MB/s]"}},"e5ceb2ac93624f2fb1d1c846daf4429f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93464a7f4d9441b0bb2a43f06b80b2c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7e7966d70dd4da2b349fe852eac8fad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8d097a8d391241adbed14148c2334573":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a63785f55a6f4e7989771f2a7f166c3f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"47aa34356529466aa342dd3e897b4a5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92df99d330b74983987c51a04309f9a3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"107482077cf2489aa75dd81aab7a6192":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_adf07566c9df4bc7b9e25eb0ce31ef1d","IPY_MODEL_dc64356b225b4ea5a5e98ac2c51e33ba","IPY_MODEL_96fff2ed6b984b71a3402e430a4e7b26"],"layout":"IPY_MODEL_88158eed25194e448fe0fbe73eec44a5"}},"adf07566c9df4bc7b9e25eb0ce31ef1d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_93d45624915b4a4296aa632497e72c85","placeholder":"​","style":"IPY_MODEL_eda2d12124594420a14569f0776dd795","value":"model.safetensors: 100%"}},"dc64356b225b4ea5a5e98ac2c51e33ba":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_957d41c4a80f41bc97b1db14e5e3df13","max":1629437147,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1d4eea2a0ea8481fa81c651d65b18b4a","value":1629437147}},"96fff2ed6b984b71a3402e430a4e7b26":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0349c740c0a8422383360b899e029e5e","placeholder":"​","style":"IPY_MODEL_7707f4a7a9ac480b9b3fd3c3cf928a07","value":" 1.63G/1.63G [00:14&lt;00:00, 37.1MB/s]"}},"88158eed25194e448fe0fbe73eec44a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93d45624915b4a4296aa632497e72c85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eda2d12124594420a14569f0776dd795":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"957d41c4a80f41bc97b1db14e5e3df13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d4eea2a0ea8481fa81c651d65b18b4a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0349c740c0a8422383360b899e029e5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7707f4a7a9ac480b9b3fd3c3cf928a07":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"egtdWTvcOeTy","executionInfo":{"status":"ok","timestamp":1716723192820,"user_tz":-120,"elapsed":3174,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"outputId":"9b483532-5441-442c-fc86-08cf1a7fe09b"},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"]}],"source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","import torch\n","import logging\n","logging.basicConfig(level=logging.ERROR)\n","# If there's a GPU available...\n","if torch.cuda.is_available():\n","\n","    # Tell PyTorch to use the GPU.\n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8AlOzas-OzjA","executionInfo":{"status":"ok","timestamp":1716723204977,"user_tz":-120,"elapsed":12173,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"outputId":"9ff40671-8ae0-42e5-dab7-312525b96777"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"]}]},{"cell_type":"code","source":["from transformers import AutoTokenizer,AutoModel\n","\n","# Load the BART tokenizer.\n","print('Loading BART tokenizer...')\n","tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":350,"referenced_widgets":["5fbb2c4b9cb04587a15bcdbf0db0991b","27c7ce7ab9704485b280ee214c5f410b","962e795bdbba4c0f8b5ec41794df47ef","5c7e7b7b326e4cc0ab30a06e0fe4c381","71c06e7be8d548fa82ebdd9a3d46ecbe","e6c0f4cf13994489b5c5b464bd3b39f7","ff4ec3dc486749a2aee99f7053b47425","52c9486d28e040de86d3fbae6df4241a","04b3821d4ef84daca8231562f80c0a99","4af3ed29094d415c941e0e194fa3ec73","049288d6594b43f5ac6bafddfe4984d1","849c112973954f6c9572da74704fa28f","f8c997705d7a476f90e0ac8d17d4a662","d029681583684c509d52b9efb2baa1d8","a8ec6c0335fd40e4a41a5c767b6b8c18","e943f87d6171475f83b97b4f8506e4d0","1b60fb49a79645abb10ca44635398c1c","d8af21b9261f4998b6cea145f03913f8","fa6939c5c21c4bc5bca32b5e53e0695f","d07a75c096ac4bd2b351ca87d0feb0fb","1177a94be5dd4ecdb408413502dd6951","97838f17e50f4e6cb6106f943bdccace","ef4b7408efa041bba4e1cfe22ecc750a","85e9eec63bd24333a19d37f0aed44a2e","65ba51616a9d467b84127c438e017f66","91bc7f438d7c42d2a08a01d4b357f22e","846bbcfff48a4bd682c9c805529d8c75","a8333cc5d74746d5b3ba70cccc660c4e","a7df96120aa04add92849c198122b809","a7103c7bdbf64ee0bcfa7af62bffd76f","15f015f07333490f8417e15b5abb17b6","66928b4933b741558bf480b192b2427d","4986dfd9775547b6b53244378a9d9225","4bd2a111cf804bbb96c16f4b0fb3822e","edfbf3c80ebf4071af271831993f0cef","41afeed0335341a9b3a8559843712d84","39919a9b549a4e0ba984b81b7e4d805d","b77fd1991e064a68978ea9065f3a4ea6","dd753754cfda4064a324910b939b674d","bef297dbd4444242908fcb5b076e6d56","5a1700d2fec2440092e5077bcf5094a3","04db08d11f644985abc5f7a001fa46e3","89cdce60ace64a01b8b39ac29b355066","4adc7602628c4020ba1bde5c0c17cc75","6cb347273df845fbbb0cc35b0cb93491","94683ee068234798ba94227ff2c3f50b","e93a5867d7f2431da1eaacc9a9c63af5","fcf8eeb4fdf34a249e3694231f90131e","e5ceb2ac93624f2fb1d1c846daf4429f","93464a7f4d9441b0bb2a43f06b80b2c5","b7e7966d70dd4da2b349fe852eac8fad","8d097a8d391241adbed14148c2334573","a63785f55a6f4e7989771f2a7f166c3f","47aa34356529466aa342dd3e897b4a5f","92df99d330b74983987c51a04309f9a3"]},"id":"C_-uxEt3O1Y_","executionInfo":{"status":"ok","timestamp":1716723208198,"user_tz":-120,"elapsed":3235,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"outputId":"375633e9-9b6a-43d8-a4e6-ec3b59d901d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading BART tokenizer...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fbb2c4b9cb04587a15bcdbf0db0991b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"849c112973954f6c9572da74704fa28f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef4b7408efa041bba4e1cfe22ecc750a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bd2a111cf804bbb96c16f4b0fb3822e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cb347273df845fbbb0cc35b0cb93491"}},"metadata":{}}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","!ls \"/content/drive/My Drive/TUDelft/NLP Group Project\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zbzQbi0F2417","executionInfo":{"status":"ok","timestamp":1716723238022,"user_tz":-120,"elapsed":29832,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"outputId":"7e6d5c3f-1dbf-4775-8185-b08f0f270e3e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","BART-large-MNLI.ipynb\t  deberta_oracle.ipynb\t    mdeberta-outputs-ORACLE\n","bart-mnli-outputs\t  deberta-outputs\t    mroberta-outputs\n","bart-mnli-outputs-ORACLE  deberta-outputs-ORACLE    mroberta-outputs-ORACLE\n","bm25\t\t\t  Jasper_DecompClaim.ipynb  Paul-Roberta-Large-MNLI.ipynb\n","claim_classifier\t  Jasper_mDeBERTa.ipynb     PLANNING\n","claimDecomp\t\t  MathBERT.ipynb\t    roberta-outputs\n","data\t\t\t  math_bert-outputs\t    roberta-outputs-ORACLE\n","deberta_bm25.ipynb\t  mdeberta-outputs\n"]}]},{"cell_type":"code","source":["import json\n","with open(\"/content/drive/MyDrive/TUDelft/NLP Group Project/data/train_claims_quantemp.json\") as f:\n","  train_data = json.load(f)\n","len(train_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wIK7glJvkXRH","executionInfo":{"status":"ok","timestamp":1716723239918,"user_tz":-120,"elapsed":1933,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"outputId":"379a3c65-9461-4904-ea2c-a9c8c50161bf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9935"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["train_data[-1]"],"metadata":{"id":"LjbPPEMKW4CM","executionInfo":{"status":"ok","timestamp":1716723239920,"user_tz":-120,"elapsed":22,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fb0e3468-8969-4a19-d54d-e5a94a978d20"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'crawled_date': '2014-03-26T10:38:09',\n"," 'country_of_origin': 'ukraine',\n"," 'label': 'False',\n"," 'url': 'https://www.stopfake.org/en/fake-commandos-from-berkut-who-refused-to-kneel-have-been-burned-alive-in-lviv/',\n"," 'lang': 'en',\n"," 'claim': 'FAKE:  Commandos from &#8220;Berkut&#8221; who refused to kneel have been burned alive in Lviv',\n"," 'doc': 'The Russian TV channel “Russia 1” aired a program called “Evil spirits of Maydan: mystic of Ukrainian mayhem”. The program, among other things, referred to the claim that two soldiers of “Berkut”, who refused to kneel in front of Lviv Maydan and recognize the current government, allegedly were burned alive. https://www.youtube.com/watch?v=SUDH0Qbjuao This was reported by the head of the so-called Russian community of Dnepropetrovsk Victor Trukhov. He says, two “Berkut” solders were put on their knees publicly and then burned in Lviv. However, contrary to this claim, a fire occured in Lviv on February 20, 2014 where people from security forces were caught in a fire. The fire started after a powerful explosion in the security forces basis, after which one officer in uniform and one in civilian clothes were pulled from the rubble. Commandos from “Berkut” kneeled on 24 February. Lviv citizens who came to the Maydan, were shouting “Shame!” and throwing small objects at security forces. To prevent any possible violence against “Berkut” soldiers, Self-Defense soldiers surrounded “Berkut”, and the priest was calming down people. \\xa0 People made the security forces to kneel on stage, and then one of the special forces soldier promised that “Berkut” will always be on the side of the people and assured that Lviv “Berkut” was not involved in the fighting in Kyiv. After the “ceremony” the commandos were taken away in the tram. Accordingly, the fire was the result of an accident and not the result of public humiliation of “Berkut”.',\n"," 'taxonomy_label': 'statistical',\n"," 'label_original': 'FAKE'}"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["import json\n","with open(\"/content/drive/MyDrive/TUDelft/NLP Group Project/data/val_claims_quantemp.json\") as f:\n","  val_data = json.load(f)\n","len(val_data)"],"metadata":{"id":"HD0NZfDFkvW1","executionInfo":{"status":"ok","timestamp":1716723240901,"user_tz":-120,"elapsed":994,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"60e5e27f-f03d-4e65-de4f-3cfe26a7145e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3084"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["\n","val_data[-1]"],"metadata":{"id":"ZT-q3h48kxdg","executionInfo":{"status":"ok","timestamp":1716723241399,"user_tz":-120,"elapsed":13,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"55d2e984-d1f2-45fd-90f5-be8d04da7f1e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'crawled_date': '2022-10-06T21:00:06',\n"," 'country_of_origin': 'usa',\n"," 'label': 'True',\n"," 'url': 'https://www.politifact.com/factchecks/2021/oct/28/randy-feenstra/biden-administration-predicted-liquid-fuel-cars-ou/',\n"," 'lang': 'en',\n"," 'claim': 'The Biden administration \"published a study concluding 4 (of) 5 new cars on the road by 2050 will still require liquid fuels.\"',\n"," 'doc': 'President Joe Biden was in Michigan’s auto industry hub on Oct. 5 when he said, \"the whole world knows that the future of the auto industry is electric.\" Rep. Randy Feenstra, R-Iowa, had a quick response, writing on Twitter: \".@POTUS no it’s not — in fact, your own administration published a study concluding 4/5 new cars on the road by 2050 will still require liquid fuels ... \"It’s past time Biden lives up to his promise to expand clean-burning #biofuels. Don’t mess with the RFS!\" Feenstra is correct about the share of cars in the United States projected to use liquid fuels. The U.S. Energy Information Administration’s 2021 Annual Energy Outlook report, which projects the nation’s environmental plans through 2050, says about 79% of new vehicle sales will be powered by liquid fuels — gasoline and blends that include up to 85% ethanol — in 2050. They accounted for 95% of sales in 2020, the report states. PolitiFact contacted Feenstra’s communications staff over 10 times for comment but did not receive a response. However, Feenstra’s tweet shared the link to his sourcing, and on page 15, the federal report notes that, while electric energy’s biggest demand growth area is transportation, the share will be small — less than 3%: \"Current laws and regulations are not projected to induce much market growth, despite continuing improvements in electric vehicles (EVs) through evolutionary market developments. Both vehicle sales and utilization (miles driven) would need to increase substantially for EVs to raise electric power demand growth rates by more than a fraction of a percentage point per year.\" And, on the report’s page 26: \"Because most light-duty vehicles have internal combustion engines, motor gasoline remains the major transportation fuel through 2050 as personal travel returns to pre-pandemic per-driver levels in the longer term.\" While 2050 is 29 years down the road, Biden did not state in Michigan a time limit on when he thinks electric power would come close to liquid fuels’ share of the automobile market. Nor did he state in detail what that future for electric power for automobiles would be. His administration’s Annual Energy Outlook reports that it is not predicting future energy use; rather it is a projection based on assumptions and methodologies that can be changed when subjected to changes in technology, demographics and resources. The report also notes that it is a response to the Department of Energy Organization Act of 1977. That law requires the U.S. Energy Information Administration to produce annual reports on trends and projections for energy use and supply, the report notes. But Feenstra was precise with his facts, and is not alone reminding Biden to support biofuels. Other Iowa congressional members, Republican and Democrat, have pushed for biofuels support, regardless of who is president. So has the biofuels industry. The reason? Iowa leads the nation when it comes to producing biofuels from farm crops. It has capacity to produce 4.6 billion gallons, double the 2.3 billion in the No. 2 state, Nebraska. While we focused this story on predicted biofuels use in vehicles, it’s worth noting that Biden promised support for biofuels in a Sept. 9, 2020, campaign statement that criticized a pre-election policy reversal by then-President Donald Trump on biofuels. Trump’s reversal led to his administration’s rejection of oil refinery industry requests to be exempt from requirements to blend certain amounts of ethanol and biodiesel in gasoline: \"A Biden-Harris Administration will fight for family farmers and revitalize rural economies — from keeping our promises to farmers by ushering in a new era of biofuels, to investing in the broadband infrastructure and rural health care access that families and communities need,\" the Biden statement read. Trump reversed course again and approved on Jan. 19, 2021, just before leaving office, three biofuel waivers for refineries. Feenstra said the Biden Administration predicts that four out of five new cars on the road will require liquid fuels in 2050 and he cites the source. That source, a report from the Biden Administration, states that 79% of vehicles on the road will require liquid fuels by 2050. We rate Feenstra’s statement to be True.',\n"," 'taxonomy_label': 'statistical',\n"," 'label_original': 'true'}"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","LE = LabelEncoder()"],"metadata":{"id":"7cYv6F4Nk4SJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_features(data):\n","  features = []\n","  evidences = []\n","\n","  for index, fact in enumerate(data):\n","    claim = fact[\"claim\"]\n","\n","\n","    feature = \"[Claim]:\"+claim+\"[Evidences]:\"+fact[\"doc\"]\n","    features.append(feature)\n","  return features\n","\n","\n"],"metadata":{"id":"8eKFjiC3i8Yx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_features = get_features(train_data)"],"metadata":{"id":"qY7Gux6Yn5rI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(train_features)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VJ5AoF54oV2t","executionInfo":{"status":"ok","timestamp":1716723243623,"user_tz":-120,"elapsed":1538,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"outputId":"6a5a59ba-8766-4f45-ff7a-ba7f65093c31"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9935"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["train_features[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":137},"id":"FHml9DUHn_bh","executionInfo":{"status":"ok","timestamp":1716723243624,"user_tz":-120,"elapsed":1228,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"outputId":"545f5c66-a902-4ef4-b3f0-70ac937a90c2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'[Claim]:In her budget speech, Nirmala Sitharaman claimed that the Government distributed 35,000 crore LED bulbs in the country.[Evidences]:Did Finance Minister Nirmala Sitharaman claim the government distributed 35,000 crore LED bulbs under the Ujala scheme? This would imply the Modi govt gave about 300 bulbs to every person in India. At least this is what is being claimed by some social media users who are sharing a screenshot from a news segment on business channel CNBC Awaaz. The photo shows Sitharaman delivering her budget speech while a caption at the bottom reads - \"35,000 crore LED bulb baantein gaye\" (35,000 crore LED bulbs were distributed). The snapshot gives the impression that Sitharaman said this sentence in her speech. Netizens are displaying shock at this whopping number believing that the finance minister\\'s statement is true. Some Congress leaders are also trolling her by sharing the screenshot of the news channel. But, India Today Anti Fake News War Room (AFWA) found that Sitharaman never said that 35,000 crore LED bulbs were distributed in the country in her speech. In fact, she said that approximately 35 crore LED bulbs were distributed under the Ujala scheme. Among many who have shared the news channel\\'s screenshot is Congress spokesperson Shobha Oza who tweeted the picture. Her tweet was retweeted more than 350 times and had over 1,500 likes by the time of writing this story. Former Cabinet Minister in Haryana government, Mahender Pratap Singh also trolled Sitharaman, believing the news to be true. There are some more verified social media users who have shared the same image of the news channel. 125 35000 LED - 280 .. , ...? pic.twitter.com/p8JKRWrMPb Sachin Chaudhary (@SChaudharyINC) July 6, 2019 Every Indian after receiving 300 LED bulbs each, as claimed by Dumb Sitharaman !! (She claims 35000 crore LED bulbs have been distributed, which means every Indian must have got approx 300 each) pic.twitter.com/7Nsz0ZYLm6 Gaurav Pandhi (@GauravPandhi) July 6, 2019 At 1:06:47, in the YouTube video of the Budget 2019 speech, one can hear Finance Minister Nirmala Sitharaman saying \"approximately 35 crore LEDs have been distributed under Ujala Yojana\". The text speech of the budget also reads \"approximately 35 crore LED bulbs\", and not \"35,000 crore LEDs\". Website of UJALA also states that till July 6, more than 35 crore LEDs have been distributed in the country. A senior editor at CNBC Awaaz spoke to us and confirmed that the wrong figure was aired due to a typo which was corrected when noticed. INDIA TODAY FACT CHECK Claim In her budget speech, Nirmala Sitharaman claimed that the Government distributed 35,000 crore LED bulbs in the country. Conclusion Sitharaman never said this in the budget speech. She stated that about 35 crore LED bulbs were distributed under UJALA scheme. JHOOTH BOLE KAUVA KAATE The number of crows determines the intensity of the lie. 1 Crow: Half True 2 Crows: Mostly lies 3 Crows: Absolutely false'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["val_features = get_features(val_data)"],"metadata":{"id":"SeHEwL3doMD6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(val_features)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DQ-ipHaHoiY-","executionInfo":{"status":"ok","timestamp":1716723243625,"user_tz":-120,"elapsed":1212,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"outputId":"165ecf9c-6b41-4aba-c2dc-02de07c5de58"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3084"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["val_features[2]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":137},"id":"Iyh75sm2ok_6","executionInfo":{"status":"ok","timestamp":1716723243627,"user_tz":-120,"elapsed":1205,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"outputId":"746fe924-c1c9-4953-ec9c-6af389124fe9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'[Claim]:Says Dino Rossi \"stripped\" health care \"from 45,000 children.\"[Evidences]:Did Dino Rossi, a Republican Washington state senator running for U.S. Congress, once take away coverage from 45,000 children? That’s what an attack ad by the House Majority PACclaimed. \"He’s back. Dino Rossi,\" the voiceover in the ad says. \"Rossi first showed up in Olympia decades ago. He went to work! Health care stripped from 45,000 children.\" They’re referencing a 2003 budget measure, back when Rossi was the state Senate Ways and Means chairman. The budget was looking bleak, so Republicans proposed large spending cuts that year. One, proposed by Rossi, was a $50 million cut in Medicaid spending. The cut would have eliminated coverage for nearly 40,000 low-income children, according to reporting at the time. But the cut didn’t fly with Democrats, and ended up being scrapped in the final budget. The final budget, which won bipartisan support, wasn\\'t a specific cut, but it did include rules changes. Those changesultimately resulted in the loss of coverage for 45,000 kids on Medicaid. While it didn’t change any of the eligibility requirements, the budget changed state Medicaid policy in a way that lead to an enrollment decline later reported by state health care agencies. We were unable to trace this measure back to Rossi himself, but he did vote in its favor. First, it ended continuous eligibility, which meant if a family’s income was over the income standard, the child’s medical coverage would end. Second, certification periods went from every 12 to every six months. A signature became necessary for renewal, ending telephone renewals. Third, it adopted new income verification requirements; while it previously accepted a family’s statements, it now verified the information provided through available databases. \"These changes were requested by the legislature and they did have the impact of reducing the children’s caseload by at least 45,000,\" said Amy Blondin, chief communications officer for the Washington State Health Care Authority. Andrew Bell, spokesman for Dino Rossi, said the changes Rossi pushed forward in the budget ensured resources were given to the neediest families. \"So it seems like the HCA is saying that that was a combination of families who were making enough money to afford healthcare themselves moved off of Medicaid coverage, and a reduction in wealthy families who were cheating the system and taking health care from poorer families,\" Bell said. \"Either way, this seems to leave more resources available for truly needy families (which is the standard we\\'ve always used).\" That’s one way to interpret the drop. Using data from the Medical Assistance Administration, the University of Washington Health Policy Analysis Program found most children leaving Medicaid weren’t ineligible in an April 2004 study. \"In fact, only about 20 to 30 percent of the children removed are ineligible for Medicaid,\" the report read. \"Most are eligible but are inhibited from getting coverage because of the administrative barriers and hurdles described above.\" When the first two rules changed back (and income verification stay put) in 2005, the numbers mostly returned to normal. Source: The Commonwealth Fund, June 2006 \"I don’t think that ‘wealthy’ families were taking advantage of the program given the bounce back in enrollment,\" said Tricia Brooks, senior fellow at Georgetown University’s Center for Children and Families. The ad said Rossi stripped health care from 45,000 children. Rossi wasn’t solely responsible for the move, but he did support it. The 2003 Washington state Senate budget measure he voted for increased administrative hurdles for children seeking Medicaid coverage. Most of the eligible children regained coverage when the rules changed back in 2005. We rate this statement Mostly True.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["train_labels = [fact[\"label\"] for fact in train_data]\n","val_labels = [fact[\"label\"] for fact in val_data]"],"metadata":{"id":"98BSyfIZontr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_labels_final = LE.fit_transform(train_labels)\n","train_labels_final"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XJYf-sJ4pHCd","executionInfo":{"status":"ok","timestamp":1716723243629,"user_tz":-120,"elapsed":1200,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"outputId":"b3235bb7-7178-4e22-f264-1bb76ed585db"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 1, 1, ..., 0, 0, 1])"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["train_labels_final[:20]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-cjPop6cpdiA","executionInfo":{"status":"ok","timestamp":1716723243630,"user_tz":-120,"elapsed":1192,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"outputId":"17981cb2-e43c-4dc5-e04b-948bfdb76fb3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 0, 1, 2, 2, 1, 1, 0])"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["val_labels_final = LE.transform(val_labels)\n","val_labels_final"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9dO586pApTPH","executionInfo":{"status":"ok","timestamp":1716723243630,"user_tz":-120,"elapsed":1185,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"outputId":"2c5b72c4-d7a7-4be9-b1a8-f988bc9045d1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 1, 2, ..., 1, 0, 2])"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["val_data[-1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sDzDVC1lnmwD","executionInfo":{"status":"ok","timestamp":1716723243633,"user_tz":-120,"elapsed":1179,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"outputId":"b38cfdf0-6361-413a-cf61-e087a70dd82b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'crawled_date': '2022-10-06T21:00:06',\n"," 'country_of_origin': 'usa',\n"," 'label': 'True',\n"," 'url': 'https://www.politifact.com/factchecks/2021/oct/28/randy-feenstra/biden-administration-predicted-liquid-fuel-cars-ou/',\n"," 'lang': 'en',\n"," 'claim': 'The Biden administration \"published a study concluding 4 (of) 5 new cars on the road by 2050 will still require liquid fuels.\"',\n"," 'doc': 'President Joe Biden was in Michigan’s auto industry hub on Oct. 5 when he said, \"the whole world knows that the future of the auto industry is electric.\" Rep. Randy Feenstra, R-Iowa, had a quick response, writing on Twitter: \".@POTUS no it’s not — in fact, your own administration published a study concluding 4/5 new cars on the road by 2050 will still require liquid fuels ... \"It’s past time Biden lives up to his promise to expand clean-burning #biofuels. Don’t mess with the RFS!\" Feenstra is correct about the share of cars in the United States projected to use liquid fuels. The U.S. Energy Information Administration’s 2021 Annual Energy Outlook report, which projects the nation’s environmental plans through 2050, says about 79% of new vehicle sales will be powered by liquid fuels — gasoline and blends that include up to 85% ethanol — in 2050. They accounted for 95% of sales in 2020, the report states. PolitiFact contacted Feenstra’s communications staff over 10 times for comment but did not receive a response. However, Feenstra’s tweet shared the link to his sourcing, and on page 15, the federal report notes that, while electric energy’s biggest demand growth area is transportation, the share will be small — less than 3%: \"Current laws and regulations are not projected to induce much market growth, despite continuing improvements in electric vehicles (EVs) through evolutionary market developments. Both vehicle sales and utilization (miles driven) would need to increase substantially for EVs to raise electric power demand growth rates by more than a fraction of a percentage point per year.\" And, on the report’s page 26: \"Because most light-duty vehicles have internal combustion engines, motor gasoline remains the major transportation fuel through 2050 as personal travel returns to pre-pandemic per-driver levels in the longer term.\" While 2050 is 29 years down the road, Biden did not state in Michigan a time limit on when he thinks electric power would come close to liquid fuels’ share of the automobile market. Nor did he state in detail what that future for electric power for automobiles would be. His administration’s Annual Energy Outlook reports that it is not predicting future energy use; rather it is a projection based on assumptions and methodologies that can be changed when subjected to changes in technology, demographics and resources. The report also notes that it is a response to the Department of Energy Organization Act of 1977. That law requires the U.S. Energy Information Administration to produce annual reports on trends and projections for energy use and supply, the report notes. But Feenstra was precise with his facts, and is not alone reminding Biden to support biofuels. Other Iowa congressional members, Republican and Democrat, have pushed for biofuels support, regardless of who is president. So has the biofuels industry. The reason? Iowa leads the nation when it comes to producing biofuels from farm crops. It has capacity to produce 4.6 billion gallons, double the 2.3 billion in the No. 2 state, Nebraska. While we focused this story on predicted biofuels use in vehicles, it’s worth noting that Biden promised support for biofuels in a Sept. 9, 2020, campaign statement that criticized a pre-election policy reversal by then-President Donald Trump on biofuels. Trump’s reversal led to his administration’s rejection of oil refinery industry requests to be exempt from requirements to blend certain amounts of ethanol and biodiesel in gasoline: \"A Biden-Harris Administration will fight for family farmers and revitalize rural economies — from keeping our promises to farmers by ushering in a new era of biofuels, to investing in the broadband infrastructure and rural health care access that families and communities need,\" the Biden statement read. Trump reversed course again and approved on Jan. 19, 2021, just before leaving office, three biofuel waivers for refineries. Feenstra said the Biden Administration predicts that four out of five new cars on the road will require liquid fuels in 2050 and he cites the source. That source, a report from the Biden Administration, states that 79% of vehicles on the road will require liquid fuels by 2050. We rate Feenstra’s statement to be True.',\n"," 'taxonomy_label': 'statistical',\n"," 'label_original': 'true'}"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["len(val_labels_final)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FardkyUtyT2B","executionInfo":{"status":"ok","timestamp":1716723243634,"user_tz":-120,"elapsed":1170,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"outputId":"24af55ef-080d-4d8f-8f21-e4e8e7411ba5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3084"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["input_ids = []\n","attention_masks = []\n","\n","for sent in train_features:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 256,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        truncation=True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","\n","    # Add the encoded sentence to the list.\n","    input_ids.append(encoded_dict['input_ids'])\n","\n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', train_features[0])\n","print('Token IDs:', input_ids[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M9xEVEmzpcSS","executionInfo":{"status":"ok","timestamp":1716723279446,"user_tz":-120,"elapsed":36969,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"outputId":"e25cef70-7f30-4fb8-dc82-87dc4f1a1537"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Original:  [Claim]:In her budget speech, Nirmala Sitharaman claimed that the Government distributed 35,000 crore LED bulbs in the country.[Evidences]:Did Finance Minister Nirmala Sitharaman claim the government distributed 35,000 crore LED bulbs under the Ujala scheme? This would imply the Modi govt gave about 300 bulbs to every person in India. At least this is what is being claimed by some social media users who are sharing a screenshot from a news segment on business channel CNBC Awaaz. The photo shows Sitharaman delivering her budget speech while a caption at the bottom reads - \"35,000 crore LED bulb baantein gaye\" (35,000 crore LED bulbs were distributed). The snapshot gives the impression that Sitharaman said this sentence in her speech. Netizens are displaying shock at this whopping number believing that the finance minister's statement is true. Some Congress leaders are also trolling her by sharing the screenshot of the news channel. But, India Today Anti Fake News War Room (AFWA) found that Sitharaman never said that 35,000 crore LED bulbs were distributed in the country in her speech. In fact, she said that approximately 35 crore LED bulbs were distributed under the Ujala scheme. Among many who have shared the news channel's screenshot is Congress spokesperson Shobha Oza who tweeted the picture. Her tweet was retweeted more than 350 times and had over 1,500 likes by the time of writing this story. Former Cabinet Minister in Haryana government, Mahender Pratap Singh also trolled Sitharaman, believing the news to be true. There are some more verified social media users who have shared the same image of the news channel. 125 35000 LED - 280 .. , ...? pic.twitter.com/p8JKRWrMPb Sachin Chaudhary (@SChaudharyINC) July 6, 2019 Every Indian after receiving 300 LED bulbs each, as claimed by Dumb Sitharaman !! (She claims 35000 crore LED bulbs have been distributed, which means every Indian must have got approx 300 each) pic.twitter.com/7Nsz0ZYLm6 Gaurav Pandhi (@GauravPandhi) July 6, 2019 At 1:06:47, in the YouTube video of the Budget 2019 speech, one can hear Finance Minister Nirmala Sitharaman saying \"approximately 35 crore LEDs have been distributed under Ujala Yojana\". The text speech of the budget also reads \"approximately 35 crore LED bulbs\", and not \"35,000 crore LEDs\". Website of UJALA also states that till July 6, more than 35 crore LEDs have been distributed in the country. A senior editor at CNBC Awaaz spoke to us and confirmed that the wrong figure was aired due to a typo which was corrected when noticed. INDIA TODAY FACT CHECK Claim In her budget speech, Nirmala Sitharaman claimed that the Government distributed 35,000 crore LED bulbs in the country. Conclusion Sitharaman never said this in the budget speech. She stated that about 35 crore LED bulbs were distributed under UJALA scheme. JHOOTH BOLE KAUVA KAATE The number of crows determines the intensity of the lie. 1 Crow: Half True 2 Crows: Mostly lies 3 Crows: Absolutely false\n","Token IDs: tensor([    0, 10975, 45699, 42645,  1121,    69,  1229,  1901,     6,   234,\n","         9856,  2331, 33922,   271,  7243,  1695,    14,     5,  1621,  7664,\n","         1718,     6,   151,  4963, 10918, 27353,    11,     5,   247, 31274,\n","        25377, 31688, 42645, 20328,  4090,   692,   234,  9856,  2331, 33922,\n","          271,  7243,  2026,     5,   168,  7664,  1718,     6,   151,  4963,\n","        10918, 27353,   223,     5,   121,   267,  2331,  3552,   116,   152,\n","           74, 25696,     5,  4698,   213, 26390,   851,    59,  2993, 27353,\n","            7,   358,   621,    11,   666,     4,   497,   513,    42,    16,\n","           99,    16,   145,  1695,    30,   103,   592,   433,  1434,    54,\n","           32,  3565,    10, 27314,    31,    10,   340,  2835,    15,   265,\n","         4238, 17826, 11614,   102,  1222,     4,    20,  1345,   924, 33922,\n","          271,  7243,  5830,    69,  1229,  1901,   150,    10,  3747,    23,\n","            5,  2576,  7005,   111,    22,  2022,     6,   151,  4963, 10918,\n","        32384, 17279,  5285,   179,  5100,   242,   113,    36,  2022,     6,\n","          151,  4963, 10918, 27353,    58,  7664,   322,    20, 24512,  2029,\n","            5,  8450,    14, 33922,   271,  7243,    26,    42,  3645,    11,\n","           69,  1901,     4,  5008, 38839,    32, 18534,  4817,    23,    42,\n","        15846,   346, 13294,    14,     5,  2879,  1269,    18,   445,    16,\n","         1528,     4,   993,  1148,   917,    32,    67, 33220,    69,    30,\n","         3565,     5, 27314,     9,     5,   340,  4238,     4,   125,     6,\n","          666,  2477,  9511, 24530,   491,  1771,  8499,    36,  8573,  8460,\n","           43,   303,    14, 33922,   271,  7243,   393,    26,    14,  1718,\n","            6,   151,  4963, 10918, 27353,    58,  7664,    11,     5,   247,\n","           11,    69,  1901,     4,    96,   754,     6,    79,    26,    14,\n","         2219,  1718,  4963, 10918, 27353,    58,  7664,   223,     5,   121,\n","          267,  2331,  3552,     4,  3687,     2])\n"]}]},{"cell_type":"code","source":["val_input_ids = []\n","val_attention_masks = []\n","\n","for sent in val_features:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 256,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        truncation=True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","\n","    # Add the encoded sentence to the list.\n","    val_input_ids.append(encoded_dict['input_ids'])\n","\n","    # And its attention mask (simply differentiates padding from non-padding).\n","    val_attention_masks.append(encoded_dict['attention_mask'])\n","# Convert the lists into tensors.\n","val_input_ids = torch.cat(val_input_ids, dim=0)\n","val_attention_masks = torch.cat(val_attention_masks, dim=0)\n","\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', val_features[0])\n","print('Token IDs:', val_attention_masks[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qZavNUqipvVV","executionInfo":{"status":"ok","timestamp":1716723292139,"user_tz":-120,"elapsed":12728,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"outputId":"52d82fb7-6acf-42a0-be2b-bae6c81c07d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original:  [Claim]:Amit Shah said Narendra Modi sleeps for 24 hours for the welfare of the poor.[Evidences]:The India Today Anti-Fake News War Room found the viral video of Amit Shah's statement was clipped and presented out of context. A short video clip of Union Home Minister Amit Shah has gone viral with the claim that at a political rally, he said that Prime Minister Narendra Modi sleeps 24 hours for the welfare of the poor. Several Twitter and Facebook users shared this video clip with captions like, “Modi ji sleeps for 24 hours”. The India Today Anti-Fake News War Room ( AFWA) found the viral video was clipped and presented out of context to give it a different meaning. In the original video, Shah can be heard saying that PM Modi thinks about the welfare of the poor 24 hours a day while “Didi” (Mamta Banerjee) wonders when her nephew would become the Chief Minister. Shah made the statement while addressing a public meeting in Chapra, West Bengal, in April 2021. The viral posts are archived here, here, and here. AFWA probe With the help of keywords searches, we discovered that the same video along with the same claim had made the rounds of social media in 2021 as well. We found the original video on the official Bharatiya Janata Party YouTube channel. It was uploaded on April 17, 2021. The title of the YouTube video read, “HM Shri Amit Shah addresses public meeting in Chapra, West Bengal.” At the 5.34-minutes mark, one can hear the unedited version of the portion of the speech that went viral. In the original video, Shah was talking about the Trinamool Congress-led government in West Bengal. Shah said “This is a government for the welfare of the nephew... Modi ji thinks about the welfare of the poor, 24 hours a day. And Didi thinks about when her nephew will become Chief Minister, 24 hours a day.” Instead of “sote” (sleep), Shah said “sochte” (thinks) in the original video. But the video was clipped to change the meaning of the sentence altogether. Thus, an old clipped video of Amit Shah has gone viral with a misleading claim. (With inputs from Sanjana Saxena) INDIA TODAY FACT CHECK Claim Amit Shah said Narendra Modi sleeps for 24 hours for the welfare of the poor. Conclusion The viral video is clipped. In the original video, Shah says PM Modi thinks about the welfare of the poor, 24 hours a day, while “Didi” (Mamata Bannerjee) wonders when her nephew would become CM. Shah was addressing a public meeting in Chapra, West Bengal, in April 2021. JHOOTH BOLE KAUVA KAATE The number of crows determines the intensity of the lie. 1 Crow: Half True 2 Crows: Mostly lies 3 Crows: Absolutely false\n","Token IDs: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"]}]},{"cell_type":"code","source":["train_labels_final = torch.tensor(train_labels_final)\n","val_labels_final = torch.tensor(val_labels_final)"],"metadata":{"id":"evSioYADp_y4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7Nk-HGh0yWwf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_labels_final.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GC7Tt92syW5Z","executionInfo":{"status":"ok","timestamp":1716723292142,"user_tz":-120,"elapsed":87,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"outputId":"f17c3769-88ef-4239-891f-119b1fd83249"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3084])"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["len(val_input_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SLCxP_iqzspd","executionInfo":{"status":"ok","timestamp":1716723292143,"user_tz":-120,"elapsed":81,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"outputId":"9476b9d5-a564-4ab8-8634-5f771d218c32"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3084"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["num_classes = len(list(set(train_labels)))\n","list(set(train_labels))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HIo3GxyUrMml","executionInfo":{"status":"ok","timestamp":1716723292143,"user_tz":-120,"elapsed":66,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"outputId":"5cd38693-fb57-4bde-b8bf-2dacce06e57e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['True', 'False', 'Conflicting']"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["num_classes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wg90AQJcrZFD","executionInfo":{"status":"ok","timestamp":1716723292144,"user_tz":-120,"elapsed":50,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"outputId":"f9f9f03e-5d3f-4fe0-9c17-d7bb9e1410b4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, random_split\n","# train_poincare_tensor = torch.tensor(poincare_embeddings_final,dtype=torch.float)\n","# difficulty_tensor = torch.tensor(difficulty_level_vectors,dtype=torch.float)\n","# Combine the training inputs into a TensorDataset.\n","dataset = TensorDataset(input_ids, attention_masks, train_labels_final)\n","val_dataset = TensorDataset(val_input_ids, val_attention_masks,val_labels_final)\n","#"],"metadata":{"id":"KOK3P-9Gra78"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"xuW19VNvaCTa"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","batch_size = 16\n","train_dataloader = DataLoader(\n","            dataset,  # The training samples.\n","            sampler = RandomSampler(dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","validation_dataloader = DataLoader(\n","            val_dataset, # The validation samples.\n","            sampler = SequentialSampler(val_dataset),\n","            batch_size = batch_size\n","        )"],"metadata":{"id":"c6ALqnRkrjBN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch import nn\n","class MultiClassClassifier(nn.Module):\n","    def __init__(self, bart_model_path, labels_count, hidden_dim=768, mlp_dim=500, extras_dim=100, dropout=0.1, freeze_bart=False):\n","        super().__init__()\n","\n","        self.bart = AutoModel.from_pretrained(bart_model_path,output_hidden_states=True,output_attentions=True)\n","        self.dropout = nn.Dropout(dropout)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(hidden_dim, mlp_dim),\n","            nn.ReLU(),\n","            # nn.Linear(mlp_dim, mlp_dim),\n","            # # nn.ReLU(),\n","            # # nn.Linear(mlp_dim, mlp_dim),\n","            # nn.ReLU(),\n","            nn.Linear(mlp_dim, labels_count)\n","        )\n","        # self.softmax = nn.LogSoftmax(dim=1)\n","        if freeze_bart:\n","            print(\"Freezing layers\")\n","            for param in self.bart.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, tokens, masks):\n","        output = self.bart(tokens, attention_mask=masks)\n","        dropout_output = self.dropout(output['last_hidden_state'][:, 0, :])\n","        # concat_output = torch.cat((dropout_output, topic_emb), dim=1)\n","        # concat_output = self.dropout(concat_output)\n","        mlp_output = self.mlp(dropout_output)\n","        # proba = self.sigmoid(mlp_output)\n","        # proba = self.softmax(mlp_output)\n","\n","        return mlp_output"],"metadata":{"id":"cKSZPwobrl8y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","# Loads BertForSequenceClassification, the pretrained BERT model with a single\n","model = MultiClassClassifier('facebook/bart-large-mnli',3, 1024,768,140,dropout=0.1,freeze_bart=False)\n","\n","# model.load_state_dict(torch.load(\"model_bert_difficulty_prediction/model_weights\"))\n","\n","# Tell pytorch to run this model on the GPU.\n","model.cuda()\n","print(next(model.parameters()).device)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["107482077cf2489aa75dd81aab7a6192","adf07566c9df4bc7b9e25eb0ce31ef1d","dc64356b225b4ea5a5e98ac2c51e33ba","96fff2ed6b984b71a3402e430a4e7b26","88158eed25194e448fe0fbe73eec44a5","93d45624915b4a4296aa632497e72c85","eda2d12124594420a14569f0776dd795","957d41c4a80f41bc97b1db14e5e3df13","1d4eea2a0ea8481fa81c651d65b18b4a","0349c740c0a8422383360b899e029e5e","7707f4a7a9ac480b9b3fd3c3cf928a07"]},"id":"CAxysrcNsFl8","executionInfo":{"status":"ok","timestamp":1716723310871,"user_tz":-120,"elapsed":18757,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"outputId":"193b2092-328b-4aa5-a325-75f8c7cc4aad"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"107482077cf2489aa75dd81aab7a6192"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"iilNEZRCsJjH"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"awQ2Y9Jb3kht","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716723311348,"user_tz":-120,"elapsed":513,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"outputId":"78bdf024-8f96-4a06-a58a-9c94a87f3c57"},"source":["optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5,\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]},{"cell_type":"code","metadata":{"id":"2Ys-M4-e3khv"},"source":["from transformers import get_linear_schedule_with_warmup\n","\n","\n","epochs = 20\n","\n","# Total number of training steps is [number of batches] x [number of epochs].\n","total_steps = len(train_dataloader) * epochs\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QrYqErOD3khx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716723311349,"user_tz":-120,"elapsed":31,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"outputId":"4c449040-de4f-4b8b-9c77-2698c72ec521"},"source":["len(train_dataloader)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["621"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"EWVSE9LM3kh0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716723311349,"user_tz":-120,"elapsed":22,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"outputId":"2d18bb33-368e-43e1-b285-823319c541a1"},"source":["1935 * 32"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["61920"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"rcvxVVi63kh3"},"source":["scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GUw3zm6g3kh5"},"source":["import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ta6zfUTa3kh7"},"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","\n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GFq9gd5kQSHb"},"source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a_nmuoSgQ5t3"},"source":["class EarlyStopping:\n","    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n","    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n","        \"\"\"\n","        Args:\n","            patience (int): How long to wait after last time validation loss improved.\n","                            Default: 7\n","            verbose (bool): If True, prints a message for each validation loss improvement.\n","                            Default: False\n","            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n","                            Default: 0\n","            path (str): Path for the checkpoint to be saved to.\n","                            Default: 'checkpoint.pt'\n","            trace_func (function): trace print function.\n","                            Default: print\n","        \"\"\"\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","        self.path = path\n","        self.trace_func = trace_func\n","    def __call__(self, val_loss, model):\n","\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model):\n","        '''Saves model when validation loss decrease.'''\n","        if self.verbose:\n","            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n","        torch.save(model.state_dict(), self.path)\n","        self.val_loss_min = val_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for param in model.bart.encoder.layers[0:5].parameters():\n","    param.requires_grad=False"],"metadata":{"id":"ugVDrHu20c8G"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E1rDO58zMfc8"},"source":["loss_func = nn.CrossEntropyLoss()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":418},"id":"6LhAy2hZ3kh9","outputId":"4118171c-39d9-4c52-a373-5a945c96250f","executionInfo":{"status":"error","timestamp":1716723313263,"user_tz":-120,"elapsed":1554,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}}},"source":["import random\n","import numpy as np\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll store a number of quantities such as training and validation loss,\n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","early_stopping = EarlyStopping(patience=2, verbose=True)\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","\n","    # ========================================\n","    #               Training\n","    # ========================================\n","\n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_accuracy = 0\n","    total_train_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to\n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questimport gensim.downloader as api\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","\n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Unpack this training batch from our dataloader.\n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids\n","        #   [1]: attention masks\n","        #   [2]: labels\n","\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        # b_poincare = batch[2].to(device)\n","        # b_difficulty = batch[3].to(device)\n","        b_labels = batch[2].to(device)\n","        # skill_labels = batch[3].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because\n","        # accumulating the gradients is \"convenient while training RNNs\".\n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()\n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        probas = model(b_input_ids,b_input_mask)\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value\n","        # from the tensor.\n","        loss = loss_func(probas, b_labels)\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        # scheduler.step()\n","        logits = probas.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        total_train_accuracy += flat_accuracy(logits, label_ids)\n","    avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n","    print(\" Train Accuracy: {0:.2f}\".format(avg_train_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)\n","\n","\n","\n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","\n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables\n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","\n","        # Unpack this training batch from our dataloader.\n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using\n","        # the `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids\n","        #   [1]: attention masks\n","        #   [2]: labels\n","        b_input_ids = batch[0].to(device)\n","\n","        b_input_mask = batch[1].to(device)\n","        # b_poincare = batch[2].to(device)\n","        # b_difficulty = batch[3].to(device)\n","        b_labels = batch[2].to(device)\n","        # skill_labels = batch[3].to(device)\n","\n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():\n","\n","            # Forward pass, calculate logit predictions.\n","\n","          logits = model(b_input_ids,b_input_mask)\n","\n","        # Accumulate the validation loss.\n","        loss = loss_func(logits, b_labels)\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","\n","\n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","    early_stopping(avg_val_loss, model)\n","    if early_stopping.early_stop:\n","      print(\"Early stopping\")\n","      break\n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","\n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","    output_dir = 'model_bart_large_oracle/'\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","\n","    print(\"Saving model to %s\" % output_dir)\n","    tokenizer.save_pretrained(output_dir)\n","    torch.save(model.state_dict(), os.path.join(output_dir, 'model_weights'))\n","\n","    output_dir = f'/content/drive/MyDrive/NLP Group Project/bart-mnli-outputs-ORACLE'\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","\n","    print(\"Saving model to %s\" % output_dir)\n","    tokenizer.save_pretrained(output_dir)\n","    torch.save(model.state_dict(), os.path.join(output_dir, 'model_weights'))\n","\n","    !rm -rf \"/content/drive/My Drive/TUDelft/ecir_compnumfacts/model_bart_large_oracle\"\n","    !mv model_roberta_large_oracle \"/content/drive/My Drive/TUDelft/ecir_compnumfacts/\"\n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======== Epoch 1 / 20 ========\n","Training...\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-46-36f193c44349>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# Perform a forward pass (evaluate the model on this training batch).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mprobas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_input_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Accumulate the training loss over all of the batches so that we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-33-12bf43821f34>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens, masks)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mdropout_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'last_hidden_state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# concat_output = torch.cat((dropout_output, topic_emb), dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m             encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1611\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[0membed_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2262\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2263\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2264\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}]},{"cell_type":"markdown","source":["# Testing"],"metadata":{"id":"06p0OxaV5_6b"}},{"cell_type":"markdown","source":["## Run Test set"],"metadata":{"id":"DiUUria4wwrU"}},{"cell_type":"code","source":["import torch\n","import json\n","from transformers import AutoTokenizer\n","from torch.utils.data import DataLoader, TensorDataset\n","from tqdm import tqdm\n","import numpy as np\n","from sklearn.metrics import accuracy_score, f1_score, classification_report, precision_recall_fscore_support\n","from collections import defaultdict, Counter\n","\n","NAME_OF_MODEL = 'bart-mnli'\n","model = MultiClassClassifier('facebook/bart-large-mnli', labels_count=3, hidden_dim=1024, mlp_dim=768, extras_dim=140, dropout=0.1)\n","model.load_state_dict(torch.load(f'/content/drive/MyDrive/TUDelft/NLP Group Project/{NAME_OF_MODEL}-outputs/model_weights'))\n","\n","if torch.cuda.is_available():\n","    model.cuda()\n","    print(\"Model moved to GPU.\")\n","else:\n","    print(\"GPU not available, using CPU.\")\n","\n","model.eval()\n","tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli', do_lower_case=True)\n","\n","with open('/content/drive/MyDrive/TUDelft/NLP Group Project/data/test_claims_quantemp.json', 'r') as file:\n","    test_data = json.load(file)\n","test_labels = [fact[\"label\"] for fact in test_data]\n","test_features = get_features(test_data)\n","label_mapping = list(LE.classes_)\n","print(\"Label mappings:\", label_mapping)\n","\n","test_labels_final = LE.fit_transform(test_labels)\n","test_taxonomy_labels = [fact[\"taxonomy_label\"] for fact in test_data]\n","\n","\n","input_ids = []\n","attention_masks = []\n","\n","for sent in test_features:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 256,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        truncation=True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","\n","    # Add the encoded sentence to the list.\n","    input_ids.append(encoded_dict['input_ids'])\n","\n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","test_labels_final = torch.tensor(test_labels_final)\n","dataset = TensorDataset(input_ids, attention_masks, test_labels_final)\n","batch_size = 16\n","test_dataloader = DataLoader(\n","            dataset,\n","            sampler = RandomSampler(dataset), # Select batches randomly\n","            batch_size = batch_size\n","        )\n","\n","loss_func = nn.CrossEntropyLoss()\n","softmax = nn.Softmax(dim=1)\n","\n","correct = []\n","incorrect = []\n","taxonomy_metrics = defaultdict(lambda: defaultdict(list))\n","overall_true_labels = []\n","overall_predictions = []\n","\n","for i, batch in enumerate(tqdm(test_dataloader, desc=\"Evaluating\")):\n","    batch = tuple(t.cuda() if torch.cuda.is_available() else t for t in batch)\n","    b_input_ids, b_input_mask, b_labels = batch\n","\n","    with torch.no_grad():\n","        logits = model(b_input_ids, b_input_mask)\n","        predicted_labels = torch.argmax(softmax(logits), dim=1)\n","          # Now, predicted labels is one hot encoded.\n","        start_index = i * batch_size\n","        for j in range(len(b_labels)):\n","          decoded_sentence = tokenizer.decode(b_input_ids[j], skip_special_tokens=True)\n","          true_label = label_mapping[b_labels[j].item()]\n","          predicted_label = label_mapping[predicted_labels[j].item()]\n","          taxonomy = test_taxonomy_labels[start_index + j]\n","          result_info = {\n","            \"Tokenizer_decoded\": decoded_sentence,\n","            \"Ground_truth\": true_label,\n","            \"Predicted_as\": predicted_label,\n","            \"Taxonomy\": taxonomy\n","          }\n","          if (predicted_labels[j] != b_labels[j]):\n","            incorrect.append(result_info)\n","          else:\n","            correct.append(result_info)\n","    # Store overall predictions and true labels for overall metrics\n","    overall_predictions.extend(predicted_labels.cpu().numpy())\n","    overall_true_labels.extend(b_labels.cpu().numpy())\n","\n","    # Store predictions for each taxonomy\n","    start_index = i * batch_size\n","    for j in range(len(b_labels)):\n","        taxonomy = test_taxonomy_labels[start_index + j]\n","        taxonomy_metrics[taxonomy]['true'].append(b_labels[j].item())\n","        taxonomy_metrics[taxonomy]['pred'].append(predicted_labels[j].item())\n","\n","# Calculate overall metrics\n","overall_macro_f1 = f1_score(overall_true_labels, overall_predictions, average='macro', zero_division=0)\n","overall_weighted_f1 = f1_score(overall_true_labels, overall_predictions, average='weighted', zero_division=0)\n","_, _, overall_f1_per_class, _ = precision_recall_fscore_support(overall_true_labels, overall_predictions, average=None)\n","\n","# Prepare to collect taxonomy-specific metrics\n","taxonomy_specific_metrics = {}\n","for taxonomy, data in taxonomy_metrics.items():\n","    macro_f1 = f1_score(data['true'], data['pred'], average='macro')\n","    weighted_f1 = f1_score(data['true'], data['pred'], average='weighted')\n","    _, _, f1_per_class, _ = precision_recall_fscore_support(data['true'], data['pred'], average=None)\n","    taxonomy_specific_metrics[taxonomy] = {\n","        'macro_f1': macro_f1,\n","        'weighted_f1': weighted_f1,\n","        'per_class_f1': f1_per_class\n","    }\n","\n","latex_row = \"Awesome-Model\"\n","for taxonomy in [\"statistical\", \"temporal\", \"interval\", \"comparison\"]:\n","    if taxonomy in taxonomy_specific_metrics:\n","        latex_row += f\" & {taxonomy_specific_metrics[taxonomy]['macro_f1']:.2f} & {taxonomy_specific_metrics[taxonomy]['weighted_f1']:.2f}\"\n","    else:\n","        latex_row += \" & - & -\"\n","\n","# Add per-class F1 scores\n","latex_row += f\" & {overall_f1_per_class[2]:.2f} & {overall_f1_per_class[1]:.2f} & {overall_f1_per_class[0]:.2f}\"\n","\n","# Add overall QUANTemp scores\n","latex_row += f\" & {overall_macro_f1:.2f} & {overall_weighted_f1:.2f} \\\\\\\\\"\n","\n","print(\"\\nLatex row:    & M-F1 & W-F1 & M-F1 & W-F1 & M-F1 & W-F1 & M-F1 & W-F1 & T-F1 & F-F1 & C-F1 & M-F1 & W-F1 \")\n","print(latex_row)\n","\n"],"metadata":{"id":"mSU7Qjo8ADv3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Save incorrectly classified for qualitative analysis"],"metadata":{"id":"Ja_RoEvv8T2d"}},{"cell_type":"code","source":["\n","correct_dump = json.dumps(correct, indent=4)\n","\n","with open(f'/content/drive/MyDrive/TUDelft/NLP Group Project/{NAME_OF_MODEL}-outputs/test_results/{NAME_OF_MODEL}_correctly_classified_test_items.json', 'w') as json_file:\n","    json_file.write(correct_dump)\n","\n","incorrect_dump = json.dumps(incorrect, indent=4)\n","\n","with open(f'/content/drive/MyDrive/TUDelft/NLP Group Project/{NAME_OF_MODEL}-outputs/test_results/{NAME_OF_MODEL}_incorrectly_classified_test_items.json', 'w') as json_file:\n","    json_file.write(incorrect_dump)"],"metadata":{"id":"A0yThthb8SQq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Confusion Matrices"],"metadata":{"id":"9NXYPw2nxAN7"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix\n","import numpy as np\n","\n","def plot_confusion_matrix(cm, class_names, title):\n","    \"\"\"\n","    Returns a matplotlib figure containing the plotted confusion matrix.\n","\n","    Args:\n","    cm (array, shape = [n, n]): a confusion matrix of integer classes\n","    class_names (array, shape = [n]): String names of the integer classes\n","    title (str): Title of the heatmap.\n","    \"\"\"\n","    # Normalize the confusion matrix.\n","    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","    fig, ax = plt.subplots(figsize=(6,6))  # Adjust to fit your needs\n","    sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap='Blues', cbar=False, ax=ax,\n","                xticklabels=class_names, yticklabels=class_names)\n","\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.title(title)\n","    plt.show()\n","    plt.close()  # Close the plot to avoid direct output\n","    return fig\n","\n","# Calculate confusion matrices and plot them\n","categories = ['statistical', 'temporal', 'interval', 'comparison', 'overall']\n","class_labels = ['Conflicting', 'False', 'True']\n","\n","# Generate and save confusion matrix plots\n","for category in categories:\n","    true_labels = taxonomy_metrics[category]['true'] if category != 'overall' else overall_true_labels\n","    pred_labels = taxonomy_metrics[category]['pred'] if category != 'overall' else overall_predictions\n","    cm = confusion_matrix(true_labels, pred_labels, labels=[0, 1, 2])\n","\n","    fig = plot_confusion_matrix(cm, class_labels, f'{NAME_OF_MODEL.upper()} {category.upper()}')\n","    fig.savefig(f'/content/drive/MyDrive/TUDelft/NLP Group Project/{NAME_OF_MODEL}-outputs/confusion_matrices/{NAME_OF_MODEL}_{category}_confusion_matrix.png')  # Save the figure\n"],"metadata":{"id":"jxVxsIpbqTfV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","\n","# Assuming overall_true_labels and overall_predictions are populated\n","true_label_counts = Counter(overall_true_labels)\n","predicted_label_counts = Counter(overall_predictions)\n","\n","# If using LabelEncoder to transform labels, ensure it's fitted as shown previously\n","label_mapping = list(LE.classes_)  # E.g., ['Conflicting', 'False', 'True']\n","\n","# Print counts for true labels\n","print(\"Counts of each class in the test set:\")\n","for label_index, count in true_label_counts.items():\n","    print(f\"{label_mapping[label_index]}: {count}\")\n","\n","# Print counts for predicted labels\n","print(\"\\nCounts of each class predicted by the model:\")\n","for label_index, count in predicted_label_counts.items():\n","    print(f\"{label_mapping[label_index]}: {count}\")\n"],"metadata":{"id":"T-bL5N3GwVJy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.load_state_dict(torch.load(\"/content/drive/MyDrive/NLP Group Project/bart-mnli-outputs-ORACLE/model_weights\"))\n","model.eval()"],"metadata":{"id":"WEjNfRIN3wko"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss_func = nn.CrossEntropyLoss()\n","softmax = nn.Softmax(dim=1)\n","\n","correct = []\n","incorrect = []\n","\n","\n","for batch in validation_dataloader:\n","\n","\n","    b_input_ids = batch[0].to(device)\n","    b_input_mask = batch[1].to(device)\n","    b_labels = batch[2].to(device)\n","\n","    with torch.no_grad():\n","\n","      logits = model(b_input_ids,b_input_mask)\n","      predicted_labels = torch.argmax(softmax(logits), dim=1)\n","\n","      # Now, predicted labels is one hot encoded.\n","      for i in range(len(b_labels)):\n","        if (predicted_labels[i] != b_labels[i]):\n","          incorrect.append(tokenizer.decode(b_input_ids[i]))\n","        else:\n","          correct.append(tokenizer.decode(b_input_ids[i]))"],"metadata":{"id":"KlkPBy5d3vM6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Printing some correctly classified documents\n","correct[0]"],"metadata":{"id":"GW_l8pbH3mC2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["incorrect[23]"],"metadata":{"id":"HdcfhwX43jqp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["correct_dump = json.dumps(correct, indent=4)\n","\n","with open('/content/drive/MyDrive/TUDelft/NLP Group Project/bart-mnli-outputs-ORACLE/bart_mnli_correct.json', 'w') as json_file:\n","    json_file.write(correct_dump)"],"metadata":{"id":"5-OPKrJG3ddU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["incorrect_dump = json.dumps(incorrect, indent=4)\n","\n","with open('/content/drive/MyDrive/TUDelft/NLP Group Project/bart-mnli-outputs-ORACLE/bart_mnli_incorrect.json', 'w') as json_file:\n","    json_file.write(incorrect_dump)"],"metadata":{"id":"xy__BpY8VOQp"},"execution_count":null,"outputs":[]}]}