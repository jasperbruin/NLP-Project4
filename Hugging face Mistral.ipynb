{"cells":[{"cell_type":"markdown","metadata":{"id":"SzJIvbi7q_zQ"},"source":["# folwoing git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":98188,"status":"ok","timestamp":1717605604378,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"},"user_tz":-120},"id":"vlGkQ3z-rBtp","outputId":"1b4fc36b-53ae-4bb2-fe4e-bf2cb6e442e7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting mistral-inference\n","  Downloading mistral_inference-1.1.0-py3-none-any.whl (21 kB)\n","Collecting fire>=0.6.0 (from mistral-inference)\n","  Downloading fire-0.6.0.tar.gz (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting mistral_common<2.0.0,>=1.0.0 (from mistral-inference)\n","  Downloading mistral_common-1.2.1-py3-none-any.whl (704 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m704.9/704.9 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: safetensors>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mistral-inference) (0.4.3)\n","Requirement already satisfied: simple-parsing>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from mistral-inference) (0.1.5)\n","Collecting xformers>=0.0.24 (from mistral-inference)\n","  Downloading xformers-0.0.26.post1-cp310-cp310-manylinux2014_x86_64.whl (222.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.7/222.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire>=0.6.0->mistral-inference) (1.16.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire>=0.6.0->mistral-inference) (2.4.0)\n","Collecting jsonschema==4.21.1 (from mistral_common<2.0.0,>=1.0.0->mistral-inference)\n","  Downloading jsonschema-4.21.1-py3-none-any.whl (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic==2.6.1 (from mistral_common<2.0.0,>=1.0.0->mistral-inference)\n","  Downloading pydantic-2.6.1-py3-none-any.whl (394 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.8/394.8 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sentencepiece==0.1.99 in /usr/local/lib/python3.10/dist-packages (from mistral_common<2.0.0,>=1.0.0->mistral-inference) (0.1.99)\n","Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.10/dist-packages (from mistral_common<2.0.0,>=1.0.0->mistral-inference) (4.12.1)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.21.1->mistral_common<2.0.0,>=1.0.0->mistral-inference) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.21.1->mistral_common<2.0.0,>=1.0.0->mistral-inference) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.21.1->mistral_common<2.0.0,>=1.0.0->mistral-inference) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.21.1->mistral_common<2.0.0,>=1.0.0->mistral-inference) (0.18.1)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==2.6.1->mistral_common<2.0.0,>=1.0.0->mistral-inference) (0.7.0)\n","Collecting pydantic-core==2.16.2 (from pydantic==2.6.1->mistral_common<2.0.0,>=1.0.0->mistral-inference)\n","  Downloading pydantic_core-2.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: docstring-parser~=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing>=0.1.5->mistral-inference) (0.16)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers>=0.0.24->mistral-inference) (1.25.2)\n","Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.10/dist-packages (from xformers>=0.0.24->mistral-inference) (2.3.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers>=0.0.24->mistral-inference) (3.14.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers>=0.0.24->mistral-inference) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers>=0.0.24->mistral-inference) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers>=0.0.24->mistral-inference) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers>=0.0.24->mistral-inference) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0->xformers>=0.0.24->mistral-inference)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0->xformers>=0.0.24->mistral-inference)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0->xformers>=0.0.24->mistral-inference)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0->xformers>=0.0.24->mistral-inference)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0->xformers>=0.0.24->mistral-inference)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0->xformers>=0.0.24->mistral-inference)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0->xformers>=0.0.24->mistral-inference)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0->xformers>=0.0.24->mistral-inference)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0->xformers>=0.0.24->mistral-inference)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0->xformers>=0.0.24->mistral-inference)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0->xformers>=0.0.24->mistral-inference)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers>=0.0.24->mistral-inference) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->xformers>=0.0.24->mistral-inference)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0->xformers>=0.0.24->mistral-inference) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0->xformers>=0.0.24->mistral-inference) (1.3.0)\n","Building wheels for collected packages: fire\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117029 sha256=04b990bb0a957b9da19f3c4892bfe9a337d8c05948309d94d3a3a69bd5fc2192\n","  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n","Successfully built fire\n","Installing collected packages: pydantic-core, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fire, pydantic, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, jsonschema, mistral_common, xformers, mistral-inference\n","  Attempting uninstall: pydantic-core\n","    Found existing installation: pydantic_core 2.18.4\n","    Uninstalling pydantic_core-2.18.4:\n","      Successfully uninstalled pydantic_core-2.18.4\n","  Attempting uninstall: pydantic\n","    Found existing installation: pydantic 2.7.3\n","    Uninstalling pydantic-2.7.3:\n","      Successfully uninstalled pydantic-2.7.3\n","  Attempting uninstall: jsonschema\n","    Found existing installation: jsonschema 4.19.2\n","    Uninstalling jsonschema-4.19.2:\n","      Successfully uninstalled jsonschema-4.19.2\n","Successfully installed fire-0.6.0 jsonschema-4.21.1 mistral-inference-1.1.0 mistral_common-1.2.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pydantic-2.6.1 pydantic-core-2.16.2 xformers-0.0.26.post1\n"]}],"source":["!pip install mistral-inference"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["3dc1ee25312b4121bd4b60684ac5f5b6","f51bdd526f634a4c859ca8e94b812e01","3f5502dcaee74f78959cb66ccc3fabc6","7b11fa44fa644394ab7ebf23bcecf974","38265b98844f4731aff9a3960deba4e4","e323a7fd764a428e97f13ad5ade8fc09","def7a199121549f9b506243a51762730","9eade225dd0147e0afbbd756f4f29d0a","757eb041793245e9ad287b44a4a01344","18ef4657bfc34f38acb3134c628e481f","1f0c451a18684add809d0d2dc410a829","1d83d99242c94e78acf170875c18af47","8d8d9bf234ac4ed889561c94457c5a05","af3dd1412c6e4e3f9dbf56899dd24da1"]},"executionInfo":{"elapsed":550,"status":"ok","timestamp":1717605604925,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"},"user_tz":-120},"id":"QaF4p2kOyfGx","outputId":"f5a60747-18cd-4bbf-9918-79962d37437f"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3dc1ee25312b4121bd4b60684ac5f5b6","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]},"metadata":{},"output_type":"display_data"}],"source":["from huggingface_hub import login\n","login()\n","# PUT IN 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"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B1-Jx743yFeV"},"outputs":[],"source":["# from huggingface_hub import snapshot_download\n","# from pathlib import Path\n","\n","# mistral_models_path = Path.home().joinpath('mistral_models', '7B-Instruct-v0.3')\n","# mistral_models_path.mkdir(parents=True, exist_ok=True)\n","\n","# snapshot_download(repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir=mistral_models_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"USLIn0aH1Ij8"},"outputs":[],"source":["# from transformers import pipeline\n","\n","\n","# chatbot = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.3\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5U4_BGCTGbUq"},"outputs":[],"source":["# messages = [\n","#     {\"role\": \"user\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n","#     {\"role\": \"assistant\", \"content\": \"Who are you?\"},\n","# ]\n","\n","\n","# response = chatbot(messages, max_new_tokens=50)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h3TN1EI8Nft9"},"outputs":[],"source":["# print(response)"]},{"cell_type":"code","execution_count":147,"metadata":{"id":"yMqjg70LOIUA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717685766708,"user_tz":-120,"elapsed":312,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"outputId":"5cec35f5-762e-439b-81a1-241a49dba91a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing complete. Check the output file.\n"]}],"source":["import json\n","\n","def load_and_process_json(file_path):\n","    # Open the file and read it as a single string\n","    with open(file_path, 'r') as file:\n","        file_content = file.read()\n","\n","    # Since the data may have multiple JSON arrays back to back, we merge them into one.\n","    # Replace '][' with ',', effectively merging all the arrays into one big array.\n","    corrected_content = file_content.replace('][', ',')\n","\n","    # Load the corrected string as JSON\n","    try:\n","        data = json.loads(corrected_content)\n","    except json.JSONDecodeError as e:\n","        print(f\"Error decoding JSON: {str(e)}\")\n","        return []\n","\n","    # Deduplicate and reindex\n","    seen_claims = set()\n","    new_data = []\n","    for entry in data:\n","        claim = entry.get('claim')\n","        if claim not in seen_claims:\n","            seen_claims.add(claim)\n","            entry['idx'] = len(new_data)\n","            new_data.append(entry)\n","\n","    return new_data\n","\n","# Usage example\n","file_path = '/content/train_decomposed3.json'\n","new_data = load_and_process_json(file_path)\n","\n","# Save the processed data\n","output_file_path = '/content/train_decomposed_cleaned.json'\n","with open(output_file_path, 'w') as outfile:\n","    json.dump(new_data, outfile, indent=4)\n","print(\"Processing complete. Check the output file.\")\n"]},{"cell_type":"markdown","metadata":{"id":"dm5LW5byQNeL"},"source":["## Code for running mistral locally"]},{"cell_type":"markdown","metadata":{"id":"jrL0VGdAQbqP"},"source":["START FROM TRAINING INDEX 1150!"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20597,"status":"ok","timestamp":1717741887298,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"},"user_tz":-120},"id":"gxRQNs0hSaw3","outputId":"bff68476-0acd-4387-fd3a-842478e2e1da"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":139,"metadata":{"id":"8XA7TW7dQPOG","executionInfo":{"status":"ok","timestamp":1717684328737,"user_tz":-120,"elapsed":2,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}}},"outputs":[],"source":["import requests\n","import json\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":148,"metadata":{"id":"G54RNUvyQij2","executionInfo":{"status":"ok","timestamp":1717685814386,"user_tz":-120,"elapsed":1,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}}},"outputs":[],"source":["import requests\n","import json\n","\n","# 1. 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\n","# 2. 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\n","# 3. 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\n","# 4. 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\n","# 5. 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\n","\n","API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.3\"\n","headers = {\"Authorization\": \"Bearer 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\"}\n","\n","def query(payload):\n","    try:\n","        response = requests.post(API_URL, headers=headers, json=payload)\n","        response.raise_for_status()\n","        return response.json()\n","    except requests.RequestException as e:\n","        return {\"error\": str(e)}\n","\n","\n","def mistral_decompose(claim, claim_type, verbose=False):\n","\n","    pfc_prompt = '''Only generate 3 subquestions or facts to verify that cover the reasoning steps required to verify the claim step-by-step. Several examples are given as follows.\n","\n","    # The claim is that Howard University Hospital and Providence Hospital are both located in Washington, D.C.\n","        fact_1 = \"Howard University Hospital is located in Washington, D.C.\"\n","        fact_2 = \"Providence Hospital is located in Washington, D.C.\"\n","\n","    # The claim is that WWE Super Tuesday took place at an arena that currently goes by the name TD Garden.\n","        answer_1 = \"Which arena the WWE Super Tuesday took place?\"\n","        fact_1 = f\"{answer_1} currently goes by the name TD Garden.\"\n","\n","    # The claim is that Talking Heads, an American rock band that was \"one of the most critically acclaimed bands of the 80's\" is featured in KSPN's AAA format.\n","        fact_1 = \"Talking Heads is an American rock band that was 'one of the most critically acclaimed bands of the 80's'.\"\n","        fact_2 = \"Talking Heads is featured in KSPN's AAA format.\"\n","\n","    # The claim is that [[CLAIM]]...\n","\n","    '''\n","\n","    # important\n","    pfc_prompt = pfc_prompt.replace('[[CLAIM]]', claim)\n","\n","    payload = {\"inputs\": pfc_prompt}\n","    # Simulating a query to an API (replace with your actual API call logic)\n","    response = query({\"inputs\": pfc_prompt})  # Your function to send the API request\n","\n","    if verbose:\n","        print(f\"Debug: Sending payload - {claim}\")\n","        if response:\n","            print(f\"Debug: Received response - {response}\")\n","\n","    # Check if the response is valid and contains expected data\n","    if response and isinstance(response, list) and \"generated_text\" in response[0]:\n","        generated_text = response[0]['generated_text']\n","        start_idx = generated_text.find(f\"The claim is that {claim}\")\n","        if start_idx != -1:\n","            start_idx += len(f\"The claim is that {claim}\")\n","            relevant_text = generated_text[start_idx:]  # Only consider text after the specific claim\n","            questions = [line.strip() for line in relevant_text.split('\\n') if \"fact_\" in line or \"answer_\" in line]\n","\n","            filtered_questions = []\n","            for q in questions:\n","                parts = q.split('=')\n","                if len(parts) >= 2:\n","                    filtered_questions.append(parts[1].strip().strip('\"'))\n","\n","            if len(filtered_questions) >= 3:\n","                return filtered_questions[:3]\n","            else:\n","                return [\"Insufficient data to generate questions.\"] * 3\n","        else:\n","            return [\"Claim section not found in the response.\"] * 3\n","    else:\n","        return [\"Error in generating questions\"] * 3\n","\n"]},{"cell_type":"code","execution_count":148,"metadata":{"id":"j9vB0W7pTLS1","executionInfo":{"status":"ok","timestamp":1717685814862,"user_tz":-120,"elapsed":1,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":149,"metadata":{"id":"Yi0zzsWMQlLd","executionInfo":{"status":"ok","timestamp":1717685815849,"user_tz":-120,"elapsed":656,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}}},"outputs":[],"source":["with open(\"/content/drive/MyDrive/TUDelft/NLP Group Project/data/test_claims_quantemp.json\") as f:\n","  test_data = json.load(f)\n","\n","with open(\"/content/drive/MyDrive/TUDelft/NLP Group Project/data/train_claims_quantemp.json\") as f:\n","  train_data = json.load(f)"]},{"cell_type":"code","execution_count":150,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UVskBNEBQnNE","executionInfo":{"status":"ok","timestamp":1717686444162,"user_tz":-120,"elapsed":628315,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"outputId":"ca7c0a3b-b610-4b8f-f8d6-8f2c7662b458"},"outputs":[{"output_type":"stream","name":"stderr","text":["  8%|▊         | 438/5637 [10:28<2:04:15,  1.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Error encountered at index 438. Process terminated. Restart from this index after resolving the issue.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["train_decomposed = []\n","counter = 0\n","error_index = None\n","\n","for i, train_claim in enumerate(tqdm(train_data[4298:10000])):\n","    claim = train_claim['claim']\n","    claim_type = train_claim['taxonomy_label']\n","    label = train_claim['label']\n","\n","    output = mistral_decompose(claim, claim_type)\n","\n","    if \"Error in generating questions\" in output:\n","        error_index = i\n","        break  # Stop the loop if error is detected\n","\n","    temp = {\n","        \"idx\": i,\n","        \"claim\": claim,\n","        \"taxonomy_label\": claim_type,\n","        \"label\": label,\n","        \"q1\": output[0],\n","        \"q2\": output[1],\n","        \"q3\": output[2]\n","    }\n","    train_decomposed.append(temp)\n","\n","    # Saving every 5 claims as before, unless an error occurs\n","    if counter % 5 == 0 and not error_index:\n","        with open(\"train_decomposed3.json\", \"a\") as file:\n","            json.dump(train_decomposed, file, indent=4)\n","        train_decomposed = []\n","\n","# Save the remaining data if no errors were detected\n","if train_decomposed and not error_index:\n","    with open(\"train_decomposed3.json\", \"a\") as file:\n","        json.dump(train_decomposed, file, indent=4)\n","\n","# Save the error index to a file for recovery\n","if error_index is not None:\n","    with open(\"error_index.txt\", \"w\") as file:\n","        file.write(str(error_index))\n","        print(f\"Error encountered at index {error_index}. Process terminated. Restart from this index after resolving the issue.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-CF9pbwSPNa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717667960896,"user_tz":-120,"elapsed":334,"user":{"displayName":"Jasper Bruin","userId":"13767118553284273610"}},"outputId":"72533d86-f43b-4082-fc07-3245ce848a1f"},"outputs":[{"output_type":"stream","name":"stdout","text":["File /content/train_decomposed3.json has been copied to /content/drive/MyDrive/TUDelft/NLP Group Project/data/train_decomposed3.json\n"]}],"source":["# import os\n","# import shutil\n","\n","# def copy_json_file(source_path, destination_dir):\n","#     \"\"\"\n","#     Copy a JSON file from a source path to a destination directory.\n","\n","#     Args:\n","#     source_path (str): The full path to the source JSON file.\n","#     destination_dir (str): The directory where the JSON file should be copied.\n","#     \"\"\"\n","#     # Check if the source file exists\n","#     if not os.path.isfile(source_path):\n","#         print(\"Error: The source file does not exist.\")\n","#         return\n","\n","#     # Ensure the file is a JSON file\n","#     if not source_path.lower().endswith('.json'):\n","#         print(\"Error: The source file is not a JSON file.\")\n","#         return\n","\n","#     # Check if the destination directory exists, create if it does not\n","#     if not os.path.exists(destination_dir):\n","#         print(f\"Notice: The destination directory {destination_dir} does not exist. Creating directory.\")\n","#         os.makedirs(destination_dir)\n","\n","#     # Define the destination file path\n","#     destination_file_path = os.path.join(destination_dir, os.path.basename(source_path))\n","\n","#     # Copy the file\n","#     shutil.copy(source_path, destination_file_path)\n","#     print(f\"File {source_path} has been copied to {destination_file_path}\")\n","\n","# # Example usage\n","# source_json_path = \"/content/train_decomposed3.json\"\n","# destination_directory = \"/content/drive/MyDrive/TUDelft/NLP Group Project/data\"\n","# copy_json_file(source_json_path, destination_directory)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l4krbhByYTPW"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"VuPaXi9SOVyA"},"source":["## PROGRAM FC claim decomposition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cPilvDWYOYmC"},"outputs":[],"source":["# def pfc_mistral_decompose(claim):\n","#     pfc_prompt = '''Only generate 3 subquestions or facts to verify that cover the reasoning steps required to verify the claim step-by-step. Several examples are given as follows.\n","\n","# # The claim is that Howard University Hospital and Providence Hospital are both located in Washington, D.C.\n","#     fact_1 = \"Howard University Hospital is located in Washington, D.C.\"\n","#     fact_2 = \"Providence Hospital is located in Washington, D.C.\"\n","\n","# # The claim is that WWE Super Tuesday took place at an arena that currently goes by the name TD Garden.\n","#     answer_1 = \"Which arena the WWE Super Tuesday took place?\"\n","#     fact_1 = f\"{answer_1} currently goes by the name TD Garden.\"\n","\n","# # The claim is that Talking Heads, an American rock band that was \"one of the most critically acclaimed bands of the 80's\" is featured in KSPN's AAA format.\n","#     fact_1 = \"Talking Heads is an American rock band that was 'one of the most critically acclaimed bands of the 80's'.\"\n","#     fact_2 = \"Talking Heads is featured in KSPN's AAA format.\"\n","\n","# # The claim is that [[CLAIM]]...\n","\n","# '''\n","\n","#     # important\n","#     pfc_prompt = pfc_prompt.replace('[[CLAIM]]', claim)\n","\n","#     messages = {\n","#         \"model\": \"mistral:7b-instruct\",\n","#         \"messages\": [\n","#             {\n","#                 \"role\": \"user\",\n","#                 \"content\": pfc_prompt\n","#             }\n","#         ]\n","#     }\n","\n","\n","#     response = requests.post(\"http://localhost:11434/api/chat\", data=json.dumps(messages))\n","#     parsed_response = [json.loads(line) for line in response.text.split(\"\\n\") if line]\n","#     complete_response = [line[\"message\"][\"content\"] for line in parsed_response]\n","#     joined_string = \"\".join(complete_response)\n","#     questions = joined_string.split('\\n')\n","\n","#     stripped_questions = []\n","\n","#     for question in questions:\n","#         start_index = question.find('\"') + 1\n","#         end_index = question.rfind('\"')\n","#         question = question[start_index:end_index]\n","#         stripped_questions.append(question)\n","\n","#     while len(stripped_questions) < 3:\n","#         stripped_questions.append(\"\")\n","#     stripped_questions = stripped_questions[:3]\n","\n","\n","#     return stripped_questions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gBng85MGOr8-"},"outputs":[],"source":["# train_decomposed = []\n","# counter = 0\n","\n","# for i, train_claim in enumerate(tqdm(train_data[:2620])):\n","#     claim = train_claim['claim']\n","#     claim_type = train_claim['taxonomy_label']\n","#     label = train_claim['label']\n","\n","#     output = pfc_mistral_decompose(claim)\n","#     temp = {\n","#         \"idx\": i,\n","#         \"claim\": claim,\n","#         \"taxonomy_label\": claim_type,\n","#         \"label\": label,\n","#         \"q1\": output[0],\n","#         \"q2\": output[1],\n","#         \"q3\": output[2]\n","#     }\n","#     train_decomposed.append(temp)\n","#     counter += 1\n","\n","#     if counter % 5 == 0:\n","#         with open(\"train_decomposed.json\", \"a\") as file:\n","#             json.dump(train_decomposed, file, indent=4)\n","#         train_decomposed = []\n","\n","# # Save the remaining data\n","# if train_decomposed:\n","#     with open(\"train_decomposed.json\", \"a\") as file:\n","#         json.dump(train_decomposed, file, indent=4)\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"18ef4657bfc34f38acb3134c628e481f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d83d99242c94e78acf170875c18af47":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f0c451a18684add809d0d2dc410a829":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38265b98844f4731aff9a3960deba4e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d8d9bf234ac4ed889561c94457c5a05","placeholder":"​","style":"IPY_MODEL_af3dd1412c6e4e3f9dbf56899dd24da1","value":"Login successful"}},"3dc1ee25312b4121bd4b60684ac5f5b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_f51bdd526f634a4c859ca8e94b812e01","IPY_MODEL_3f5502dcaee74f78959cb66ccc3fabc6","IPY_MODEL_7b11fa44fa644394ab7ebf23bcecf974","IPY_MODEL_38265b98844f4731aff9a3960deba4e4"],"layout":"IPY_MODEL_e323a7fd764a428e97f13ad5ade8fc09"}},"3f5502dcaee74f78959cb66ccc3fabc6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_757eb041793245e9ad287b44a4a01344","placeholder":"​","style":"IPY_MODEL_18ef4657bfc34f38acb3134c628e481f","value":"Your token has been saved in your configured git credential helpers (store)."}},"757eb041793245e9ad287b44a4a01344":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b11fa44fa644394ab7ebf23bcecf974":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f0c451a18684add809d0d2dc410a829","placeholder":"​","style":"IPY_MODEL_1d83d99242c94e78acf170875c18af47","value":"Your token has been saved to /root/.cache/huggingface/token"}},"8d8d9bf234ac4ed889561c94457c5a05":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9eade225dd0147e0afbbd756f4f29d0a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"af3dd1412c6e4e3f9dbf56899dd24da1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"def7a199121549f9b506243a51762730":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e323a7fd764a428e97f13ad5ade8fc09":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"f51bdd526f634a4c859ca8e94b812e01":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_def7a199121549f9b506243a51762730","placeholder":"​","style":"IPY_MODEL_9eade225dd0147e0afbbd756f4f29d0a","value":"Token is valid (permission: read)."}}}}},"nbformat":4,"nbformat_minor":0}